{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "898b2c7e-7be9-43fd-90a9-00fa38862eac",
   "metadata": {},
   "source": [
    "# Pytorch Deeplearning Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828627cd-c2c1-4fd5-b1c5-68cc2d432b36",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37469f1-80d6-43ba-a527-4e118fa56ae7",
   "metadata": {},
   "source": [
    "This turotial introduces a general approach to training deep learning models with PyTorch. It is divided into the following parts:\n",
    "1. Introduction (the current section)\n",
    "2. Introduction to PyTorch and Tensor\n",
    "3. Piepline of Training a Deep Learning Model\n",
    "4. Dataset\n",
    "5. Tokenizer\n",
    "6. Vocabulary\n",
    "7. Label Mapping\n",
    "8. Model\n",
    "9. Loss Function\n",
    "10. Optimizer\n",
    "11. Data Loader\n",
    "12. Training, Validation, Testing and Prediction\n",
    "13. Complete Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0c39be-c2b6-4c0a-97a5-8b52ef2fec83",
   "metadata": {},
   "source": [
    "## 2. Introduction to PyTorch and Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190aae91-f9d8-44d8-a33e-f81b9d313bfd",
   "metadata": {},
   "source": [
    "### 2.1 PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31366bb-774d-40be-8990-ccf2f1a0cf1e",
   "metadata": {},
   "source": [
    "Pytorch is an open source machine learning framework for training deep learning models. Other similar frameworks include Tensorflow, Keras and Jax.\n",
    "\n",
    "The download command of the latest PyTorch version can be found on its [homepage](https://pytorch.org/). Several info should be considered.\n",
    "+ PyTorch Build: Some latest PyTorch versions.\n",
    "+ Your OS: Your operating system.\n",
    "+ Package: Download method. Conda and Pip are generally used.\n",
    "+ Language: Python in most cases.\n",
    "+ Compute Platform: GPU (CUDA), CPU or others.\n",
    "\n",
    "After selecting the above info, run the command provided in \"Run this Command\" to download pytorch.  \n",
    "E.g. Stable (1.12.0), Linux, Pip, Python, CPU\n",
    "```bash\n",
    "pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "\n",
    "Note that `torchvision` and `torchaudio` are task-specific package for vision (e.g. computer vision) and audio (e.g. audio recognition). For NLP, they are not needed and can be removed, resulting in the following command:\n",
    "```bash\n",
    "pip3 install torch --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "\n",
    "After downloading `torch`, check if it has been downloaded correctly with the following **Python code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effe0ad4-c37c-4f6f-9c79-7fdcc19421db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neko\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1aa3b3-2f8a-42e5-8e2e-67943e0c4d5a",
   "metadata": {},
   "source": [
    "If nothing is printed after executing the above line of code, the `torch` package is downloaded successfully.\n",
    "\n",
    "A bunch of tutorials for training deep learning models with PyTorch can be found [here](https://pytorch.org/tutorials/). They include many topics including NLP, CV and so on.\n",
    "And the PyTorch documentation is [here](https://pytorch.org/docs/stable/index.html). Check it out if you are not familiar with certain classes, methods, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf0d351-1e72-4915-9af7-ffc036461c2a",
   "metadata": {},
   "source": [
    "### 2.2 `torchtext`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a833d-6512-4a63-bcd5-4de7c04dfec2",
   "metadata": {},
   "source": [
    "`torchtext`, similar to `torchvision` and `torchaudio`, is a task-specific package for texts. It is useful for NLP tasks. `torchtext` can be downloaded with the following command:\n",
    "```bash\n",
    "pip install torchtext\n",
    "```\n",
    "\n",
    "<!-- Note that the version of `torchtext` should be compatible with `torch`. But don't worry, when downloading `torchtext`, the correct version of `torch` will also be downloaded. -->\n",
    "\n",
    "After downloading `torchtext`, check if it has been downloaded correctly with the following **Python code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dbbe10e-71e9-4376-a4ea-26c1bdd86a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b1703-f3b8-43d7-9d25-ec568dfa4cbf",
   "metadata": {},
   "source": [
    "Again, if nothing is printed after executing the above line of code, the `torchtext` package is downloaded successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d633ba-b065-4f0b-be3d-dea7651d582c",
   "metadata": {},
   "source": [
    "### 2.3 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1287e507-859c-4a79-9763-9ef180b9a20b",
   "metadata": {},
   "source": [
    "Tensors are basic units in PyTorch. A tensor is like a Python **list** or an Numpy **ndarray**. **A tensor is analogous to a matrix in any dimension that supports many deep learning operations (e.g. matrix operations, running on gpu, automatic differentiation, etc.).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e54cb7-b7b2-4cff-a308-e96946ea28aa",
   "metadata": {},
   "source": [
    "### 2.3.1 Tensor Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94580f00-99a2-4a8b-b6e0-2b4784dcaea1",
   "metadata": {},
   "source": [
    "There are a lot of ways to create a tensor. See the examples below. More examples can be found [here](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06490bbf-16dd-435c-8585-1c1b5edf744d",
   "metadata": {},
   "source": [
    "#### 2.3.1.1 Creating a Tensor from a Python List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d282adb6-d890-492d-930a-4efcae1ea9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "[[1, 2], [3, 4]]\n",
      "\n",
      "tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "]\n",
    "tensor = torch.tensor(data)\n",
    "\n",
    "print(f\"data:\\n{data}\\n\")\n",
    "print(f\"tensor:\\n{tensor}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32413f3b-24f6-4229-8bc3-f46a331091db",
   "metadata": {},
   "source": [
    "#### 2.3.1.2 Creating a Tensor from a Numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8142318b-ca28-41ca-a401-bd40d2d256af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "[[1, 2], [3, 4]]\n",
      "\n",
      "numpy array:\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      "tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Creating a numpy.ndarray.\n",
    "import numpy as np\n",
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "]\n",
    "np_array = np.array(data)\n",
    "\n",
    "# numpy.ndarray -> torch.tensor.\n",
    "tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(f\"data:\\n{data}\\n\")\n",
    "print(f\"numpy array:\\n{np_array}\\n\")\n",
    "print(f\"tensor:\\n{tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3f8bc-bfa4-40e3-8642-8e4a611d69e0",
   "metadata": {},
   "source": [
    "## 2.3.2 Tensor Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67792c4e-ca26-4d74-a3b0-b01bec88771e",
   "metadata": {},
   "source": [
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e59ac20c-4d38-433e-b61b-9a7fe98fab82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor is:\n",
      "torch.Size([3, 2])\n",
      "\n",
      "Datatype of the tensor is:\n",
      "torch.int64\n",
      "\n",
      "Device that the tensor is stored on is:\n",
      "cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a python list.\n",
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "tensor = torch.tensor(data)\n",
    "\n",
    "print(f\"Shape of the tensor is:\\n{tensor.shape}\\n\")\n",
    "print(f\"Datatype of the tensor is:\\n{tensor.dtype}\\n\")\n",
    "print(f\"Device that the tensor is stored on is:\\n{tensor.device}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a112a4-de22-499b-b6af-f926e32b08ae",
   "metadata": {},
   "source": [
    "### 2.3.3 Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247278a-530f-4900-953e-13c3489405a2",
   "metadata": {},
   "source": [
    "Over 100 tensor operations are supported by PyTorch, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling. Some tensor operations are presented below. More detailed descriptions can be found [here](https://pytorch.org/docs/stable/torch.html). **Note: you DON'T have to memorize all operations. Search what you need in the documentation or with Google / Baidu**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ea86f-864d-4188-9e04-b74a57f4368e",
   "metadata": {},
   "source": [
    "#### 2.3.3.1 Moving a Tensor from CPU to GPU\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using `.to()` method (after checking for GPU availability with `torch.cuda.is_available`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0341acb-0a3d-4540-9a84-187c0c555c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")  # Move the tensor to gpu.\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b892ae1-2c00-495e-8b80-391b3a18475e",
   "metadata": {},
   "source": [
    "#### 2.3.3.2 Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cd9564-7158-42ab-80c1-b9fd8b39d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "First row:\n",
      "tensor([1, 2])\n",
      "\n",
      "First column:\n",
      "tensor([1, 3, 5])\n",
      "\n",
      "Last column:\n",
      "tensor([2, 4, 6])\n",
      "\n",
      "After changing value:\n",
      "tensor([[1, 0],\n",
      "        [3, 0],\n",
      "        [5, 0]])\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "tensor = torch.tensor(data)\n",
    "print(f\"tensor:\\n{tensor}\\n\")\n",
    "\n",
    "print(f\"First row:\\n{tensor[0]}\\n\")\n",
    "print(f\"First column:\\n{tensor[:, 0]}\\n\")\n",
    "print(f\"Last column:\\n{tensor[:, -1]}\\n\")\n",
    "\n",
    "tensor[:,1] = 0\n",
    "print(f\"After changing value:\\n{tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24f92c0-1470-4f7d-b6e2-8eb904c4ffeb",
   "metadata": {},
   "source": [
    "#### 2.3.3.3 Converting Single-Element Tensors to Python Numerical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd1861-1ac3-4787-9432-11e92c8f54d8",
   "metadata": {},
   "source": [
    "If you have a one-element tensor (e.g. `tensor([233])`, you can convert it to a Python numerical value using the `item` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0e47d20-fae3-46ca-be9a-403605ef5200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single element tensor:\n",
      "tensor([233])\n",
      "\n",
      "numerical value:\n",
      "233\n",
      "\n",
      "single element tensor:\n",
      "tensor([[[233]]])\n",
      "\n",
      "numerical value:\n",
      "233\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_elem_tensor = torch.tensor([233])\n",
    "print(f\"single element tensor:\\n{single_elem_tensor}\\n\")\n",
    "numerical_val = single_elem_tensor.item()\n",
    "print(f\"numerical value:\\n{numerical_val}\\n\")\n",
    "\n",
    "single_elem_tensor = torch.tensor([[[233]]])\n",
    "print(f\"single element tensor:\\n{single_elem_tensor}\\n\")\n",
    "numerical_val = single_elem_tensor.item()\n",
    "print(f\"numerical value:\\n{numerical_val}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105be22-9692-4407-a21b-3630b3b0451a",
   "metadata": {},
   "source": [
    "#### 2.3.3.4 Arithmetic Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c853b5c-2ee1-4192-8a25-e2f4671a59ad",
   "metadata": {},
   "source": [
    "**Matrix multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65092f76-accb-4ae4-bae0-50000b3e1402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "tensor @ tensor:\n",
      "tensor([[ 5, 11, 17],\n",
      "        [11, 25, 39],\n",
      "        [17, 39, 61]])\n",
      "\n",
      "tensor @ tensor:\n",
      "tensor([[ 5, 11, 17],\n",
      "        [11, 25, 39],\n",
      "        [17, 39, 61]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "tensor = torch.tensor(data)\n",
    "print(f\"tensor:\\n{tensor}\\n\")\n",
    "\n",
    "y1 = tensor @ tensor.T\n",
    "print(f\"tensor @ tensor:\\n{y1}\\n\")\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "print(f\"tensor @ tensor:\\n{y2}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204fb73-12d1-418b-9f8a-e19e3718327f",
   "metadata": {},
   "source": [
    "**Element-wise multiplication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b082b0f9-5c21-426c-8f22-50e0ac3fb9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor:\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "tensor * tensor:\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16],\n",
      "        [25, 36]])\n",
      "\n",
      "tensor * tensor:\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16],\n",
      "        [25, 36]])\n",
      "\n",
      "tensor * 233:\n",
      "tensor([[ 233,  466],\n",
      "        [ 699,  932],\n",
      "        [1165, 1398]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "]\n",
    "tensor = torch.tensor(data)\n",
    "print(f\"tensor:\\n{tensor}\\n\")\n",
    "\n",
    "z1 = tensor * tensor\n",
    "print(f\"tensor * tensor:\\n{z1}\\n\")\n",
    "z2 = tensor.mul(tensor)\n",
    "print(f\"tensor * tensor:\\n{z2}\\n\")\n",
    "\n",
    "# Element-wise multiplication + broadcasting.\n",
    "z3 = tensor * 233\n",
    "print(f\"tensor * 233:\\n{z3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67cff3-19b6-49b2-8213-b46b74b4469d",
   "metadata": {},
   "source": [
    "## 3. Piepline of Training a Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37608e6-7cbb-49f7-a508-fef94cb6c7bc",
   "metadata": {},
   "source": [
    "In this turotial, we will train a deep learning model for **text classification**. It is modified from the AG News classification task.\n",
    "+ [Input] A piece of news.\n",
    "+ [Output] Category of the news (one of Business, Sci/Tech, Sports, World).\n",
    "\n",
    "Data sample:\n",
    "+ [Input] Editors' Picks: What do you think of the iMac's newest design? From the introduction of the first Macintosh computer through to the release of the iPod, Apple Computer has earned a reputation for cutting-edge industrial design. Does the newest iMac live up to that reputation?\n",
    "+ [Output] Sci/Tech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b24c06-4678-49b3-83fc-451e03379b41",
   "metadata": {},
   "source": [
    "### 3.1 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7362a2f6-3192-42d1-b945-036fe524bb3c",
   "metadata": {},
   "source": [
    "![pipeline](./pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edfa40-eb38-431d-989e-7795e5d07a25",
   "metadata": {},
   "source": [
    "**Pseudo code** for the pipeline is as follows. Details are omitted for brief demonstration.\n",
    "```python\n",
    "train_dataset = Dataset(path_of_training_data)\n",
    "dev_dataset = Dataset(path_of_dev_data)\n",
    "test_dataset = Dataset(path_of_test_data)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "vocab = Vocab()\n",
    "label_mapping = make_label_mapping()\n",
    "\n",
    "model = Model()\n",
    "\n",
    "loss_function = LossFunction()\n",
    "optimizer = Optimizer()\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "dev_loader = DataLoader(dev_dataset)\n",
    "test_loader = DataLoader(test_dataset)\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "best_model = None\n",
    "for i in range(num_epochs):\n",
    "    train(train_loader, tokenizer, vocab, model, loss_function, optimizer, label_mapping)\n",
    "\n",
    "    valid_loss = valid(dev_loader, tokenizer, vocab, model, loss_function, label_mapping)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "test(test_loader, tokenizer, vocab, best_model, label_mapping)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c53c9d9-1dbe-475b-8483-aaa97b60c2ed",
   "metadata": {},
   "source": [
    "### 3.2 Required Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9da799-7939-4b03-b57e-158b657f1c56",
   "metadata": {},
   "source": [
    "According to the pipeline above, seven components are required to training a deep learning model for text classification:\n",
    "1. **Dataset**: producing input texts and output labels.\n",
    "2. **Tokenizer**: tokenizing texts.\n",
    "3. **Vocabulary**: converting tokens to integral ids.\n",
    "4. **Label mapping**: converting classes to ids.\n",
    "5. **Model**\n",
    "6. **Loss function**\n",
    "7. **Optimizer**\n",
    "\n",
    "Note that when training, the model often receives **a batch** of training examples instead of one training example. Thus, a data loader is needed for sampling training examples from the dataset, constructing batches, and so on.\n",
    "\n",
    "8. **Data loader**: Sampling training examples and constructing batches.\n",
    "\n",
    "The following sections will detail how to construct each component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8554aae0-595d-43d7-9a3a-fbf1af3d96dd",
   "metadata": {},
   "source": [
    "## 4. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c0900-cf73-4170-8adf-f187ca14c060",
   "metadata": {},
   "source": [
    "### 4.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114c849-3124-41c8-b901-4a135df10271",
   "metadata": {},
   "source": [
    "First, we will create datasets for the training, development and test data. Datasets are created for\n",
    "+ reading data from data files\n",
    "+ storing data\n",
    "+ producing data for training\n",
    "\n",
    "What we will accomplish in this section can be explained by the following pseudo code:\n",
    "```python\n",
    "train_dataset = Dataset(path_of_training_data)\n",
    "dev_dataset = Dataset(path_of_dev_data)\n",
    "test_dataset = Dataset(path_of_test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531537fc-aff2-42f2-b4e5-656a46bc99eb",
   "metadata": {},
   "source": [
    "### 4.2 Required Methods for Implementing a PyTorch Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7472930-bee5-44ee-8b22-b6ba59c2c260",
   "metadata": {},
   "source": [
    "In PyTorch, a custom dataset corresponds to a Python **class**. The dataset class must implement three methods: \n",
    "+ `__init__`: reading data from data files.\n",
    "+ `__len__`: returning the length, i.e. number of examples, of the dataset.\n",
    "+ `__getitem__`: producing a (input, output) pair, given an index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f17909c-9293-4267-9835-6d16551c0ab9",
   "metadata": {},
   "source": [
    "### 4.3 Implementation of the Three Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8dd30-4ce4-4065-b731-0c1eb44fbc97",
   "metadata": {},
   "source": [
    "Now let's create a dataset class for the text classification task. As mentioned above, it should contain three methods `__init__`, `__len__` and `__getitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70e5c3aa-932a-40ac-a4f4-ba5a8ca9b250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"Dataset for AG News.\n",
    "\n",
    "    :param fp: File path of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fp: str):\n",
    "        ...\n",
    "\n",
    "    def __len__(self):\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a3b6f-eb8f-4061-969c-6dcd1662eb99",
   "metadata": {},
   "source": [
    "#### 4.3.1 Implementing `__init__`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22f733-6415-4327-b3e1-0cbf42d0014b",
   "metadata": {},
   "source": [
    "First, we'll write a method `read` for reading data from the data file. The method receives the path of the data file, from which it reads data from. And it returns a list of texts (model inputs) and a list of labels (model outputs).\n",
    "\n",
    "Specifically, the data files (`train.csv`, `dev.csv` and `test.csv`) used in this turotial are in csv (comma separated values) format. Each line contains news category, news title and news body, separated by commas. Some examples are as follows:\n",
    "+ World,\"Bush, Kerry Tentatively OK Three Debates (AP)\",\"AP - The campaigns of President Bush and Sen. John Kerry tentatively have agreed to a series of three debates that both sides hope will give them momentum in the closing weeks of the presidential election campaign, a person familiar with the debate negotiations said Sunday night.\"\n",
    "+ Sports,Roddick to Lead U.S. Against Belarus (AP),AP - Andy Roddick and the rest of the U.S. Davis Cup team figure it's about time the country reclaimed the championship.\n",
    "+ Sci/Tech,IMPlanet Weekly News Break,\"The free, hosted MSN Spaces service is Microsoft #39;s first consumer foray into providing a blogging platform. Spaces will enable MSN members to maintain a blog and control it with some sophisticated features that are not typical of all blogging services.\"\n",
    "\n",
    "We concatenate the news title and news body as text (model input), and use the news category as label (model output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fe3496b-38d4-4900-9549-0a740107ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from typing import Tuple, List\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"Dataset for AG News.\n",
    "\n",
    "    :param fp: File path of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fp: str):\n",
    "        self.texts, self.labels = self.read(fp)\n",
    "\n",
    "    def __len__(self):\n",
    "        ...\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        ...\n",
    "    \n",
    "    @classmethod\n",
    "    def read(cls, fp: str) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Obtain texts and labels from the data file.\n",
    "\n",
    "        :param fp: File path of the data.\n",
    "        :return: texts and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        texts = []\n",
    "        labels = []\n",
    "        with open(fp, encoding=\"utf-8\") as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            for line in csv_reader:  # Read each line in the csv file.\n",
    "                label, title, body = line  # Each field in the line is split by the csv reader automatically.\n",
    "\n",
    "                texts.append(f\"{title} {body}\")  # Concatenate the title and the body as text.\n",
    "                labels.append(label)\n",
    "\n",
    "        return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395049f-601c-400d-b951-1bebdacd0dfe",
   "metadata": {},
   "source": [
    "#### 4.3.2 Implementing `__len__`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e7546-720b-483f-94f7-e0903626b465",
   "metadata": {},
   "source": [
    "Next, let's implement the `__len__` method. It's generally implemented as the length of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0c5e63-cdd4-4f14-90d5-b69c5fa4a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __len__(self) -> int:\n",
    "    return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e78e810-d71c-43d1-a0e3-f888e21981cb",
   "metadata": {},
   "source": [
    "#### 4.3.3 Implementing `__getitem__`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed248d-dfb7-47cb-83b0-07a561d54e29",
   "metadata": {},
   "source": [
    "Now there is only one method left: `__getitem__`. This method receives an index and returns a pair (input, output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5246ac5e-4721-4f08-b730-a76f2c73cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx: int) -> Tuple[str, str]:\n",
    "    text = self.texts[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb3b11-cdeb-4de5-b710-8722c8e7ebc3",
   "metadata": {},
   "source": [
    "### 4.4 Complete Implementation for the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74be9acf-7ce5-4acb-9eee-19f43f92a4ba",
   "metadata": {},
   "source": [
    "Done! The complete implementation for the dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccac071b-3ae1-4d7c-840f-96d997134491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"Dataset for AG News.\n",
    "\n",
    "    :param fp: File path of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fp: str):\n",
    "        self.texts, self.labels = self.read(fp)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[str, str]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return text, label\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, fp: str) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Obtain texts and labels from the data file.\n",
    "\n",
    "        :param fp: File path of the data.\n",
    "        :return: texts and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        texts = []\n",
    "        labels = []\n",
    "        with open(fp, encoding=\"utf-8\") as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            for line in csv_reader:\n",
    "                label, title, body = line\n",
    "\n",
    "                # Concatenate the title and the body as text.\n",
    "                texts.append(f\"{title} {body}\")\n",
    "                labels.append(label)\n",
    "\n",
    "                # TODO: Some operations in collate_func can be placed here.\n",
    "\n",
    "        return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf95e60-c9fc-4c3b-a2d5-83628fdcdfb0",
   "metadata": {},
   "source": [
    "Let's check if we've implement correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43ad28ab-48c9-4153-8c78-73d55220d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input of the first example in the training data:\n",
      "Bush, Kerry Tentatively OK Three Debates (AP) AP - The campaigns of President Bush and Sen. John Kerry tentatively have agreed to a series of three debates that both sides hope will give them momentum in the closing weeks of the presidential election campaign, a person familiar with the debate negotiations said Sunday night.\n",
      "\n",
      "Model output of the first example in the training data:\n",
      "World\n",
      "\n",
      "Length of the training data:\n",
      "114000\n",
      "\n",
      "Model input of the first example in the training data:\n",
      "Bush, Kerry Tentatively OK Three Debates (AP) AP - The campaigns of President Bush and Sen. John Kerry tentatively have agreed to a series of three debates that both sides hope will give them momentum in the closing weeks of the presidential election campaign, a person familiar with the debate negotiations said Sunday night.\n",
      "\n",
      "Model output of the first example in the training data:\n",
      "World\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = AGNewsDataset(fp=\"../train.csv\")\n",
    "\n",
    "# Check the read method in __init__.\n",
    "print(f\"Model input of the first example in the training data:\\n{train_data.texts[0]}\\n\")\n",
    "print(f\"Model output of the first example in the training data:\\n{train_data.labels[0]}\\n\")\n",
    "\n",
    "# Check the __len__ method.\n",
    "print(f\"Length of the training data:\\n{len(train_data)}\\n\")\n",
    "\n",
    "# Check the __getitem__ method.\n",
    "print(f\"Model input of the first example in the training data:\\n{train_data[0][0]}\\n\")\n",
    "print(f\"Model output of the first example in the training data:\\n{train_data[0][1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bc42c-d23d-4d1c-a8ef-83265d51a36a",
   "metadata": {},
   "source": [
    "### 4.5 Creating Datasets for Training/Development/Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67502442-51d0-4cf3-a604-d21b7c8f13b5",
   "metadata": {},
   "source": [
    "After the check, let's create datasets for training data, development data and test data, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce2cba4b-bfec-4835-9688-ada19c0469fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AGNewsDataset(fp=\"../train.csv\")\n",
    "dev_dataset = AGNewsDataset(fp=\"../dev.csv\")\n",
    "test_dataset = AGNewsDataset(fp=\"../test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd088e-e005-40d3-a28b-bf1adec42073",
   "metadata": {},
   "source": [
    "## 5. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56498bcd-6102-484a-95b8-e5e1434fb491",
   "metadata": {},
   "source": [
    "### 5.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14680fa5-ac44-48dd-b8b2-eb90e6fd0762",
   "metadata": {},
   "source": [
    "A tokenizer is for splitting text into tokens. Many tokenizers are available, including `word_tokenize` in `nltk`, `spacy` and so on. Note that some tokenizers will **lowercase** all the tokens when tokenizing, while some will not. Lowercasing tokens can reduce the vocabulary size and the number of embedding parameters, which is beneficial for some tasks.\n",
    "\n",
    "In this section, we create a tokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8e352d-92d9-40bb-9cfd-d52a17adb538",
   "metadata": {},
   "source": [
    "### 5.2 Multiple Ways for Tokenizer Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a298cd98-7161-4333-9c6e-3f1fa1c3f117",
   "metadata": {},
   "source": [
    "Tokenizers can be created in many ways. Following is two ways to construct a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0a4cd6f-996d-4a6c-9f74-e3b37b66f55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized using torchtext.data.get_tokenizer('basic_english'): ['i', \"'\", 'am', 'learning', 'to', 'train', 'a', 'deeplearning', 'model', '.']\n",
      "Tokenized using nltk.tokenize.word_tokenize: [\"I'am\", 'learning', 'to', 'train', 'a', 'deeplearning', 'model', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I'am learning to train a deeplearning model.\"\n",
    "\n",
    "# Creating a tokenizer with torchtext.\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokens = tokenizer(text)\n",
    "print(f\"Tokenized using torchtext.data.get_tokenizer(\\'basic_english\\'): {tokens}\")\n",
    "\n",
    "# Creating a tokenizer with nltk.\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Tokenized using nltk.tokenize.word_tokenize: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bf4c4-7af8-460a-9e74-37539d03a9f2",
   "metadata": {},
   "source": [
    "### 5.3 Creating a Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c947c6bf-498f-4697-b8d5-e63ebdc94fc4",
   "metadata": {},
   "source": [
    "In this turotial we construct a tokenizer with `torchtext`. This tokenizer will lowercase all tokens. It's ok for text classification in the turotial as cases won't affect the category of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5513112-1925-4e17-a761-bde260babe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e0f15-512d-495a-a7f3-40390357ba6d",
   "metadata": {},
   "source": [
    "## 6. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348be44d-bf17-4634-afa7-a8b5341b7cd4",
   "metadata": {},
   "source": [
    "### 6.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ee8f6-8831-4ece-9567-f8bc729c0d22",
   "metadata": {},
   "source": [
    "A vocabulary stores all the **tokens that the model understands**. It is also responsible for **converting tokens into token ids**. A token id is for obtaining the embedding of the corresponding token. Sometimes we should create a vocabulary by ourselves, while sometimes need not (e.g. when we use pre-trained embeddings, the vocab is determined by the vocab of the pre-training).\n",
    "\n",
    "**Note that in this turotial we don't distinguish words and tokens for simplicity. They are interchangable in this turotial**\n",
    "\n",
    "In this turotial, we create a vocab manually, as the pseudo code below:\n",
    "```python\n",
    "def make_vocab():\n",
    "    ...\n",
    "\n",
    "vocab = make_vocab()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54c98e-3e12-42ab-a5ed-3506c6f9193f",
   "metadata": {},
   "source": [
    "### 6.2 Three Steps for Creating a Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960cdb7-cb33-4754-a528-fafd8b77cc13",
   "metadata": {},
   "source": [
    "A vocab can be created in three steps:\n",
    "\n",
    "1. Obtaining all the tokens from the data. In common practice, the vocab is made from the **training data**, as it is for training the model. The development data and the test data are for evaluating the performance of the model, and are not appropriate for vocab construction.\n",
    "2. Making a token:count mapping.\n",
    "3. Making the vocab with (1) the token:count mapping and (2) a `min_freq` threshold. All the tokens with a frequency greater than the threshold will be considered, and all those with a frequency less than the threshold will be discarded. The `min_freq` threshold is useful for reducing vocab size. A smaller vocab size results in a smaller embedding lookup table, requiring less parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af6203-eb5f-4da4-afa6-fd4bab1b9c8e",
   "metadata": {},
   "source": [
    "### 6.3 Implementation of Each Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77e52b-2a07-42fe-8d70-7e74ed830d7b",
   "metadata": {},
   "source": [
    "#### 6.3.1 Obtaining all Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f19b141-a715-46bf-8e98-49a7d5249d5d",
   "metadata": {},
   "source": [
    "Obtaining all the tokens from the training data is quite simple. Previously we have created the training dataset. We can access all texts in the training data with its `texts` attribute. Here's the first ten lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a161b26-394c-4e68-a93a-c1f7d1ea999e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bush, Kerry Tentatively OK Three Debates (AP) AP - The campaigns of President Bush and Sen. John Kerry tentatively have agreed to a series of three debates that both sides hope will give them momentum in the closing weeks of the presidential election campaign, a person familiar with the debate negotiations said Sunday night.',\n",
       " \"Roddick to Lead U.S. Against Belarus (AP) AP - Andy Roddick and the rest of the U.S. Davis Cup team figure it's about time the country reclaimed the championship.\",\n",
       " 'IMPlanet Weekly News Break The free, hosted MSN Spaces service is Microsoft #39;s first consumer foray into providing a blogging platform. Spaces will enable MSN members to maintain a blog and control it with some sophisticated features that are not typical of all blogging services.',\n",
       " 'U.S., Militants Battle in Central Baghdad (AP) AP - A gunbattle between U.S. forces and militants erupted in central Baghdad on Friday, witnesses said.',\n",
       " 'In the frame OPINIONS were split in the Australian camp over the future of Brett Lee last night following a maverick selection push by captain Ricky Ponting.',\n",
       " \"Pedestrian Ban Reversed A Prince George's County appeals board has voted to overturn a ban on pedestrians on the streets and sidewalks around FedEx Field.\",\n",
       " 'New York sues Saudi Arabia for 9/11 WASHINGTON: New York and New Jersey authorities have said they will join a lawsuit suing Saudi Arabia, Osama bin Laden and other Islamic institutions that allegedly raised money for terrorism.',\n",
       " 'U.S. Marines Train Niger Counter-Terror Force (Reuters) Reuters - U.S. marines have trained an\\\\elite force in the West African country of Niger to root out\\\\and kill al Qaeda-linked militants Washington fears may be\\\\roaming ungoverned swathes of the Sahara desert.',\n",
       " 'Salvation Army donations are down Charles Burnett was ringing a bell in front of the Wal-Mart in Fairmount. It was Burnetts first day, but the Salvation Army has stationed volunteers by red kettles for more than a hundred years.',\n",
       " \"A Rousing Comeback For Home of the Beatles Two decades ago, when this seaport city was mired in a deep and seemingly endless recession, Mike Byrne, a local musician and businessman, sought public funds to help build an exhibition dedicated to Liverpool's most famous native sons, the Beatles.\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b2cfb6-663c-4486-b1ab-86e79646cf58",
   "metadata": {},
   "source": [
    "Therefore, it is not hard to obtain all tokens from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31d93989-2b3e-4c10-9fa2-b836b9cb41b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bush', ',', 'kerry', 'tentatively', 'ok', 'three', 'debates', '(', 'ap', ')', 'ap', '-', 'the', 'campaigns', 'of', 'president', 'bush', 'and', 'sen', '.', 'john', 'kerry', 'tentatively', 'have', 'agreed', 'to', 'a', 'series', 'of', 'three', 'debates', 'that', 'both', 'sides', 'hope', 'will', 'give', 'them', 'momentum', 'in', 'the', 'closing', 'weeks', 'of', 'the', 'presidential', 'election', 'campaign', ',', 'a', 'person', 'familiar', 'with', 'the', 'debate', 'negotiations', 'said', 'sunday', 'night', '.', 'roddick', 'to', 'lead', 'u', '.', 's', '.', 'against', 'belarus', '(', 'ap', ')', 'ap', '-', 'andy', 'roddick', 'and', 'the', 'rest', 'of', 'the', 'u', '.', 's', '.', 'davis', 'cup', 'team', 'figure', 'it', \"'\", 's', 'about', 'time', 'the', 'country', 'reclaimed', 'the', 'championship', '.']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [\n",
    "    token\n",
    "    for text in train_dataset.texts\n",
    "    for token in tokenizer(text)\n",
    "]\n",
    "\n",
    "# Check it by printing the first 100 tokens:\n",
    "print(all_tokens[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a52a4-1aa9-4bfb-84f0-6371164422c6",
   "metadata": {},
   "source": [
    "#### 6.3.2 Making a Token:Count Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd95259-f876-4f0d-9af5-7dcc5c8a0e4b",
   "metadata": {},
   "source": [
    "Now let's make the token:count mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e87882cd-35d8-4b52-9d6b-119b7f9d83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {}\n",
    "for token in all_tokens:\n",
    "    try:\n",
    "        counter[token] += 1\n",
    "    except KeyError:\n",
    "        counter[token] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1382ae-a35d-401c-b28a-f150b66f55c3",
   "metadata": {},
   "source": [
    "An equivalent and more efficient way is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f46bd8ac-ba9b-4466-8e8c-d9d68f0b452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bush: 3403\n",
      ",: 157171\n",
      "kerry: 1335\n",
      "tentatively: 40\n",
      "ok: 246\n",
      "three: 4537\n",
      "debates: 84\n",
      "(: 39039\n",
      "ap: 15264\n",
      "): 38741\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Have a look at the first 10 items:\n",
    "for token, count in list(counter.items())[:10]:\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3086996c-9190-4d26-85d3-6619eccb70e2",
   "metadata": {},
   "source": [
    "Let's sort them by frequency. It's not necessay though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea798bb0-5500-42a4-882f-1e962946470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".: 214545\n",
      "the: 193701\n",
      ",: 157171\n",
      "to: 113276\n",
      "a: 104552\n",
      "of: 92908\n",
      "in: 90710\n",
      "and: 65455\n",
      "s: 58627\n",
      "on: 53678\n"
     ]
    }
   ],
   "source": [
    "counter = dict(sorted(\n",
    "    counter.items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    "))\n",
    "\n",
    "# Have a look at the first 10 items:\n",
    "for token, count in list(counter.items())[:10]:\n",
    "    print(f\"{token}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c3887-e971-42fd-8ba1-92a34f62aa93",
   "metadata": {},
   "source": [
    "#### 6.3.3 Creating a Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ec641-f79b-4bcd-b315-965d7c85b69d",
   "metadata": {},
   "source": [
    "After making the token:count mapping, we can construct a vocab. The `torchtext` package provides us a convenient function `torchtext.vocab.vocab` for making vocab. Let's first of all learn its usage from [here](https://pytorch.org/text/stable/vocab.html?highlight=vocab#torchtext.vocab.vocab).\n",
    "\n",
    "`torchtext.vocab.vocab` requires four params. Let's focus on the first two:\n",
    "+ ordered_dict: Ordered Dictionary mapping tokens to their corresponding occurance frequencies.\n",
    "+ min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
    "\n",
    "The first param `ordered_dict` is the token:count mapping we just created. And `min_freq` is the threshold mentioned earlier, let's set it to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a876af1-e5b5-4588-9e1c-a146545704d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab as vc\n",
    "min_freq = 5\n",
    "vocab = vc(\n",
    "    ordered_dict=counter,\n",
    "    min_freq=min_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f91365-4fb7-44c1-b041-e982d76ab7b4",
   "metadata": {},
   "source": [
    "Done! Now let's try to use the vocab to convert tokens to ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b776b1c-c2ba-4aec-a1e1-884cc286bb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'am', 'learning', 'to', 'train', 'a', 'deeplearning', 'model', '.']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Token deeplearning not found and default index is not set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Tokens -> token ids\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mvocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(token_ids)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchtext\\vocab\\vocab.py:35\u001b[0m, in \u001b[0;36mVocab.forward\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `lookup_indices` method\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m        The indices associated with a list of `tokens`.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Token deeplearning not found and default index is not set"
     ]
    }
   ],
   "source": [
    "text = \"I'am learning to train a deeplearning model.\"\n",
    "\n",
    "# Tokenize the text.\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)\n",
    "\n",
    "# Tokens -> token ids\n",
    "token_ids = vocab(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807a1c3-8ba5-491e-bbf5-8afbcf67a708",
   "metadata": {},
   "source": [
    "Oops! A runtime error is raised. It says the token \"deeplearning\" is not found and the default index is not set. The reason is that \"deeplearning\" is not in vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0422aec5-9734-4b08-921f-193206b9ed49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'deeplearning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stoi\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdeeplearning\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'deeplearning'"
     ]
    }
   ],
   "source": [
    "vocab.get_stoi()[\"deeplearning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fba77-530a-4fc3-b844-c376b64d7786",
   "metadata": {},
   "source": [
    "A **out-of-vocabulary (OOV)** token should be represented by a certain symbol, such as `<unk>`. We should also specify an index for the `<unk>` symbol. First, insert the unknown token to the vocab. Second, tell the vocab that `<unk>` is the default symbol, so all OOV token will be replaced by the `<unk>` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0826a98b-fec1-42e0-afd0-da7acedc14ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<unk>\"\n",
    "unk_idx = 0\n",
    "\n",
    "vocab.insert_token(\n",
    "    token=unk_token,\n",
    "    index=unk_idx\n",
    ")\n",
    "vocab.set_default_index(index=unk_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414cde9-15c0-43fa-ad47-26f9527c001a",
   "metadata": {},
   "source": [
    "Now let's run the previous code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cca38193-4e9a-432c-a297-a5a17705ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'am', 'learning', 'to', 'train', 'a', 'deeplearning', 'model', '.']\n",
      "[275, 16, 1915, 4754, 4, 1933, 5, 0, 2083, 1]\n"
     ]
    }
   ],
   "source": [
    "text = \"I'am learning to train a deeplearning model.\"\n",
    "\n",
    "# Tokenize the text.\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)\n",
    "\n",
    "# Tokens -> token ids\n",
    "token_ids = vocab(tokens)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e869193-5430-4653-a0c1-ab84a4cb3cb3",
   "metadata": {},
   "source": [
    "It works!  The text is first tokenized into tokens by the tokenizer. Then the tokens are converted into token ids by the vocab. The OOV token \"deeplearning\" is treated as `<unk>`, whose token id is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b344f-5e10-43a5-b8d2-7be3119010b6",
   "metadata": {},
   "source": [
    "### 6.4 Complete Implementation for Vocab Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffab2b-63e7-4dc4-9c95-af5f5b72e62e",
   "metadata": {},
   "source": [
    "Let's wrap the code into a function `make_vocab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06b291c1-cb9c-45e7-bddd-807be1544209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Callable\n",
    "\n",
    "def make_vocab(\n",
    "        texts: List[str], tokenizer: Callable,\n",
    "        min_freq: int = 1, unk_token: str = \"<unk>\", unk_idx: int = 0,\n",
    ") -> torchtext.vocab.Vocab:\n",
    "    \"\"\"Make a vocabulary from the specified texts.\n",
    "\n",
    "    :param texts: Texts for making vocab.\n",
    "    :param tokenizer: Tokenizer for tokenizing texts.\n",
    "    :param min_freq: Min frequency of the words to be added to the vocab.\n",
    "    :param unk_token: Unknown token.\n",
    "    :param unk_idx: Unknown token index.\n",
    "    :return: Constructed vocab.\n",
    "    \"\"\"\n",
    "\n",
    "    def make_token_count_mapping(texts) -> Dict[str, int]:\n",
    "        \"\"\"Make a mapping {token: count}\"\"\"\n",
    "\n",
    "        all_tokens = [\n",
    "            token\n",
    "            for text in texts\n",
    "            for token in tokenizer(text)\n",
    "        ]\n",
    "        counter = Counter(all_tokens)\n",
    "        counter = dict(sorted(\n",
    "            counter.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        ))\n",
    "\n",
    "        return counter\n",
    "\n",
    "    vocab = vc(\n",
    "        ordered_dict=make_token_count_mapping(texts=texts),\n",
    "        min_freq=min_freq,\n",
    "    )\n",
    "    vocab.insert_token(\n",
    "        token=unk_token,\n",
    "        index=unk_idx\n",
    "    )\n",
    "    vocab.set_default_index(index=unk_idx)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6d212-3a12-4bdf-89b6-aafe6654223d",
   "metadata": {},
   "source": [
    "### 6.5 Creating a Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6482e-893c-49aa-828c-0ff397d80b3d",
   "metadata": {},
   "source": [
    "Now, we can construct a vocab with the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86074203-ee1f-4bfe-ad98-60d83a2319f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "vocab = make_vocab(\n",
    "    texts=train_dataset.texts,\n",
    "    tokenizer=tokenizer,\n",
    "    min_freq=min_freq,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79e5da-1c09-436c-a228-d217b63c7d57",
   "metadata": {},
   "source": [
    "## 7. Label Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9703a10-ceff-4955-abf6-43ce89556636",
   "metadata": {},
   "source": [
    "### 7.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b9fcfe-9535-468e-89e1-c6319ddcb40c",
   "metadata": {},
   "source": [
    "Textual labels classes (Business, Sci/Tech, Sports, World) should be mapped to integral ids for model training. In this section, we create a mapping that maps each texutal class to an integral id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eff7a6-2ab0-4f1a-92af-0e4fa9de4d69",
   "metadata": {},
   "source": [
    "### 7.2 Two Steps for Making the Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b9a7c6-3455-4c69-aa11-2096d65b9633",
   "metadata": {},
   "source": [
    "Only two steps are needed to create the mapping:\n",
    "1. Obtaining all textual label classes from the **training data** and removing duplicates.\n",
    "2. Making the mapping with the deduplicated label classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a611c5c-b13e-40b6-8299-fcbb17cfc837",
   "metadata": {},
   "source": [
    "### 7.3 Implementation of the Two Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f67c36-b47c-49fe-98c2-4d3cee3ee507",
   "metadata": {},
   "source": [
    "First, let's obtain all textual labels with the `labels` attribute of the training dataset, followed by deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c15bf495-0a4f-4bab-88c6-f2fe75a40f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sports', 'Business', 'Sci/Tech', 'World']\n"
     ]
    }
   ],
   "source": [
    "classes = list(set(train_dataset.labels))\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20696c-40d6-4e53-a66f-e77086da3741",
   "metadata": {},
   "source": [
    "Then, we can make the mapping, which is represented by a Python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6041d9b1-51ef-4c94-81df-707cd289d4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sports': 0, 'Business': 1, 'Sci/Tech': 2, 'World': 3}\n"
     ]
    }
   ],
   "source": [
    "class_index_mapping = {\n",
    "    class_: idx\n",
    "    for idx, class_ in enumerate(classes)\n",
    "}\n",
    "\n",
    "print(class_index_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6cd66a-5657-4a66-9f19-28cbd11f4617",
   "metadata": {},
   "source": [
    "With the constructed mapping `class_index_mapping`, we can map texutal labels into label ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f92ec62-9abd-48a5-86e5-4eff2695d073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_index_mapping[\"World\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80c402e-e195-4a4f-ad02-e9771801c28a",
   "metadata": {},
   "source": [
    "### 7.4 Complete Implementation for Making the Label Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3801f-c84d-403d-870c-ab970a0eb50c",
   "metadata": {},
   "source": [
    "Let's wrap the code into a function `make_class_index_mapping()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f231f4cd-13c6-4844-a43c-3fe641323f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_class_index_mapping(labels: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"Make a mapping that maps the classes to integral indices.\n",
    "\n",
    "    :param labels: Label strings.\n",
    "    :return: Label indices.\n",
    "    \"\"\"\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    class_index_mapping = {\n",
    "        class_: idx\n",
    "        for idx, class_ in enumerate(classes)\n",
    "    }\n",
    "\n",
    "    return class_index_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5162d-2c04-485d-af27-8cf83766c366",
   "metadata": {},
   "source": [
    "### 7.5 Creating a Label Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd49022-7401-446b-a4c0-7a8abaa0ad97",
   "metadata": {},
   "source": [
    "Now, we can make the label mapping with the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35731c08-f6fc-4863-bb8e-a553152059c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_index_mapping = make_class_index_mapping(labels=train_dataset.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9f2b6-45db-4a3c-acff-f93a66752d2f",
   "metadata": {},
   "source": [
    "## 8. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8ac5-bf1d-4cb4-8d4f-b2e1aee9ec23",
   "metadata": {},
   "source": [
    "### 8.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5fa1c-def2-4059-9842-3b5d9599852c",
   "metadata": {},
   "source": [
    "This section introduces how to construct a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02732c7-25be-4262-9346-ef3b4634ee7b",
   "metadata": {},
   "source": [
    "### 8.2 Required Methods for Implementing a PyTorch Deeplearning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28306b3d-9fa8-4c20-bd3d-538f47eb6275",
   "metadata": {},
   "source": [
    "We define our neural network by subclassing `nn.Module` of PyTorch, and initialize the neural network layers in __init__. Every `nn.Module` subclass implements the operations on input data in the `forward` method. In other words, two methods are required:\n",
    "+ `__init__`: defining components of the network.\n",
    "+ `forward`: dpecifying how the inputs are processed by each component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf90f71-ff31-4655-86fa-84e7b532212c",
   "metadata": {},
   "source": [
    "### 8.3 Implementation of the Two Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085358f-a7e9-4ae6-9a6c-8c456faf9aa3",
   "metadata": {},
   "source": [
    "Now let's create a model class for the text classification task. In this turotial, we use a single layer RNN as model structure. It contains three layers:\n",
    "1. **Embedding layer** for converting token ids to embeddings. The size of the embedding lookup table is (`vocab_len`, `embed_dim`), where `vocab_len` is the length of the vocab and `embed_dim` is the dimension of the embeddings. In this tutorial, we set `embed_dim` to 50.\n",
    "2. **RNN layer** for extracting textual info. In this tutorial, we set the hidden dimension `hidden_dim` of the RNN layer to 50.\n",
    "3. **Linear layer** for decoding. The input dimension of the linear layer is `hidden_dim` of the RNN layer, and the output dimension equals to the number of news categories `class_num`, which is 4 in this tutorial.\n",
    "\n",
    "Receiving the input (a batch of token ids), the network processes it as follows:\n",
    "1. The embedding layer converts the token ids to embeddings.\n",
    "2. The RNN layer encodes the embeddings sequentially, and outputs the last hidden state.\n",
    "3. The linear layer transforms the hidden state into a four-element vector. Each elem in the vector represents the prob of the correcponding class predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2636e6-6fd5-48f4-ba5e-6f96ddaffc3b",
   "metadata": {},
   "source": [
    "As mentioned above, the model class should contain two methods `__init__` and `forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebfd8591-465b-450f-90d0-852ae14c0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class RNNTextClassifier(nn.Module):\n",
    "    \"\"\"A text classifier based on RNN.\n",
    "\n",
    "    :param emb_len: Embedding dimension.\n",
    "    :param hid_dim: Dimension of the RNN hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_len: int, class_num: int, embed_dim: int, hidden_dim: int):\n",
    "        super(RNNTextClassifier, self).__init__()\n",
    "\n",
    "        ...\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9f039-c44a-45c1-aa8b-9de86174acdb",
   "metadata": {},
   "source": [
    "#### 8.3.1 Implementing `__init__`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e682150-ca06-4e0e-8281-9bd204b33502",
   "metadata": {},
   "source": [
    "First, let's implement the `__init__` method. The arguments it receives include `vocab_len`, `embed_dim`, `hidden_dim` and `class_num`, as described above. We use [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding) for creating the lookup table, [nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN) for creating the RNN layer, and [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear) for the linear layer. \n",
    "\n",
    "Note that the construction method of **each neural layer** requires **different sets of arguments**. **Read the documentation** to learn more! For example, for constructing an embedding layer, we use `nn.Embedding`. We should read its [doc](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding) to figure out which arg represents the length of the vocab, and which arg represents the dimension of the embeddings. From the doc, we see that `num_embeddings` denotes *size of the dictionary of embeddings*, i.e., vocab length, and that `embedding_dim` denotes *the size of each embedding vector*. Therefore, we pass `vocab_len` to the arg `num_embeddings` and pass `embed_dim` to the arg `embedding_dim`. Similarly, for the RNN and the linear layer, we first read their docs, and then pass values to the corresponding args."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5532ea9c-1e97-4ec7-a783-152fcc69de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, vocab_len: int, class_num: int, embed_dim: int, hidden_dim: int):\n",
    "    super(RNNTextClassifier, self).__init__()\n",
    "\n",
    "    self.embed_dim = embed_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "\n",
    "    self.embedding = nn.Embedding(\n",
    "        num_embeddings=vocab_len,\n",
    "        embedding_dim=self.embed_dim\n",
    "    )\n",
    "    self.rnn = nn.RNN(\n",
    "        input_size=self.embed_dim,\n",
    "        hidden_size=self.hidden_dim,\n",
    "        batch_first=True\n",
    "    )\n",
    "    self.linear = nn.Linear(\n",
    "        in_features=self.hidden_dim,\n",
    "        out_features=class_num\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c5380-0e88-45a2-899c-1f80cebf7ac5",
   "metadata": {},
   "source": [
    "#### 8.3.2 Implementing `forward`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274eb1f4-e59d-41c9-a3df-7cf04229abaf",
   "metadata": {},
   "source": [
    "Now let's implement the `forward` method, which determines how the input `x` should be processed. When implementing the `forward` method, two things should be clear:\n",
    "1. The input and output of each layer.\n",
    "2. The dimension of the inputs and outputs.\n",
    "\n",
    "Again, **search the documentation** to know all we need.\n",
    "\n",
    "Specifically, let's start from the input `x`. Generally it's a tensor of size (batch_size, text_len). (As a reminder, we can use `x.size()` to obtain the size of `x`.)\n",
    "\n",
    "First, `x` is fed into the embedding layer. From the [doc](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding) of `nn.Embedding`, we can learn about the input and output description of the layer from the **Shape** section:\n",
    "+ Input: (), IntTensor or LongTensor of arbitrary shape containing the indices to extract\n",
    "+ Output: (,H), where * is the input shape and H=`embedding_dim`\n",
    "\n",
    "According to the description, the input `x` with size (batch_size, text_len) will be transformed to (batch_size, text_len, embed_dim). Therefore, the size of `embeddings` is (batch_size, text_len, embed_dim).\n",
    "\n",
    "Then we should focus on the RNN layer. It receives the `embeddings` as input. Read the **Inputs** and **Outputs** sections of the [doc](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN) for details. For the model used in this tutorial, only the final hidden state is required. The final hidden state corresponds to the second elem of the outputs (`last_hidden` in the code), whose size is (1, batch_size, hidden_dim) according to the doc. The 1 here is the default value for the layer number in the construction method of `nn.RNN`. The first dimension (with the size of 1) in `last_hidden` is redundant, so we remove this dimension with the `squeeze` method, resulting in the dimension of (batch_size, hidden_dim).\n",
    "\n",
    "Similarly, read the [doc](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear) of `nn.Linear` to learn its usage.\n",
    "\n",
    "*More for `last_hidden = last_hidden.squeeze(dim=0)`: the original `last_hidden` is (1, batch_size, hidden_dim). If we pass it directly to the linear layer, we will obtain `y` which is the output of the whole model with the size (1, batch_size, class_num). However, the model output (at least for the task here) requires the size to be (batch_size, class_num). Therefore, we squeeze the dimension of `last_hidden` in advance. In addition, we can squeeze `y` instead of squeezing `last_hidden`, which will produce the same result.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2052544f-1b7a-4f08-b1b2-cbe3ded87fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "    # x: (batch_size, text_len)\n",
    "    \n",
    "    embeddings = self.embedding(x)\n",
    "    # embeddings: (batch_size, text_len, embed_dim)\n",
    "    \n",
    "    _, last_hidden = self.rnn(embeddings)\n",
    "    # last_hidden: (1, batch_size, hidden_dim)\n",
    "    last_hidden = last_hidden.squeeze(dim=0)\n",
    "    # last_hidden: (batch_size, hidden_dim)\n",
    "    \n",
    "    y = self.linear(last_hidden)\n",
    "    # y: (batch_size, class_num)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad1334a-1ea9-4179-8acc-ef2636b0e2b5",
   "metadata": {},
   "source": [
    "### 8.4 Complete Implementation for the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b06d57-f31e-4f1a-a3f7-6d32ab62e9a4",
   "metadata": {},
   "source": [
    "Let's wrap the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "def3b2ed-e6b3-4146-a018-c2ded0620738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTextClassifier(nn.Module):\n",
    "    \"\"\"A text classifier based on RNN.\n",
    "\n",
    "    :param emb_len: Embedding dimension.\n",
    "    :param hid_dim: Dimension of the RNN hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_len: int, class_num: int, embed_dim: int, hidden_dim: int):\n",
    "        super(RNNTextClassifier, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_len,\n",
    "            embedding_dim=self.embed_dim\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=class_num\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        _, last_hidden = self.rnn(embeddings)\n",
    "        last_hidden = last_hidden.squeeze(dim=0)\n",
    "        \n",
    "        y = self.linear(last_hidden)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f7266d-945e-4948-a503-342d83d36782",
   "metadata": {},
   "source": [
    "### 8.5 Creating a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee982c9e-76a1-4d31-b513-b1f78570b146",
   "metadata": {},
   "source": [
    "Now that the model class has been implemented, let's create a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe5f1947-f179-4252-9896-958ce6839dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 50\n",
    "hidden_dim = 50\n",
    "model = RNNTextClassifier(\n",
    "    vocab_len=len(vocab),\n",
    "    class_num=len(class_index_mapping),\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a547ae5-f84f-479b-9f02-b0cb5074beee",
   "metadata": {},
   "source": [
    "## 9. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6170d5-fe5d-4007-85a1-ee5bd2e77c83",
   "metadata": {},
   "source": [
    "### 9.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c44bb0-6ae0-4946-9a65-65af99dfd359",
   "metadata": {},
   "source": [
    "In this section, we specify a loss function for loss calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b2495-e143-49e3-8258-e5678034f2c2",
   "metadata": {},
   "source": [
    "### 9.2 Loss Functions Provided by PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0e646-aff0-4ab9-977d-2a30076a31be",
   "metadata": {},
   "source": [
    "A bunch of loss functions are implemented by PyTorch, including [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) (usually for regression), [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) (usually for classification) and so on. See all the loss functions available [here](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9c75ba-05f8-4c2e-a749-a10cfdf39ab3",
   "metadata": {},
   "source": [
    "### 9.3 Creating a Loss Function (Criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe0edf-9b60-4fc3-82f2-ef5542b5c554",
   "metadata": {},
   "source": [
    "Creating a loss function (criterion) is straight-forward. Nothing more is needed to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5fa348db-ff7c-4b81-aaad-846df75e95e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99f6195-8fe2-40a7-acd1-421f148de917",
   "metadata": {},
   "source": [
    "## 10. Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bed03-a029-4baa-aae9-205eebd801b3",
   "metadata": {},
   "source": [
    "### 10.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8907ac50-0379-4469-ba6d-1590cd17c508",
   "metadata": {},
   "source": [
    "In this section, we specify a optimizer for optimizing model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d02427-3929-4991-b30f-ac65846393ec",
   "metadata": {},
   "source": [
    "### 10.2 Optimizers Provided by PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba457ac0-8d91-41df-8293-fc62cec05969",
   "metadata": {},
   "source": [
    "Similarly, there are many optimizers provided by PyTorch, such as [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD), [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam), [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop) and so on. See all [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce29dba-6f38-40d3-b279-96a367acf591",
   "metadata": {},
   "source": [
    "### 10.3 Creating an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbacd1a1-5174-4345-9c6d-c18cb5224854",
   "metadata": {},
   "source": [
    "The simplest way to create an optim is by specifying the params of the model `model.parameters()` to optimize. Here we use the `Adam` optim, with a learning rate of 1e-3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "803ab324-2561-474f-914c-e10a49416d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=lr,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62701a40-f5e1-4160-99de-5c82b562b69e",
   "metadata": {},
   "source": [
    "## 11. Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4ff43-09eb-40ae-aeb4-56e945f7cb29",
   "metadata": {},
   "source": [
    "### 11.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1a65d2-cc2c-4f9e-9fb1-7e207d9eb51b",
   "metadata": {},
   "source": [
    "Til now, we've prepared many modules for training a deep learning model, including: a dataset class for storing and obtaining data, a tokenizer and a vocab for processing texts, a label mapping for processing labels, a deep learning model, a loss func and an optimizer. We are almost ready for training the model. However, there's one more thing to consider: loading data.\n",
    "\n",
    "When training, the model receives a **batch** of training data instead of a single one. Therefore, there are several things we should take into account:\n",
    "+ How many data are there in a batch? In other word, what is the batch size?\n",
    "+ How to sample data from the dataset to form a batch? Should we randomly choose some or sequentially choose some?\n",
    "+ How do we collate the sampled data (a list of training examples) to form a batch?\n",
    "\n",
    "In this section, we introduce how to construct batches, i.e., how to load data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a3295-7604-4195-8f53-3e7a78743c90",
   "metadata": {},
   "source": [
    "### 11.2 Loading Data with `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cda7d3-a07d-446a-a327-18d57fc5f041",
   "metadata": {},
   "source": [
    "We can easily specify how the data are to be loaded with the `DataLoader` class of PyTorch. Again, first read the [doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to learn its usage. From the doc, we see that there are a bunch of params that we can configure. For now, let's only focus on four of them: `dataset`, `batch_size`, `shuffle` and `collate_fn`.\n",
    "\n",
    "The `dataset` arg, needless to say, specifies the dataset from which data are to be loaded. When training the model, we should load data from the training set. However, when we validate the model performance during training, we should load data from the dev set. And when we test the trained model after training, we should load data from the test set. Therefore, we must explicitly tell the loader from which dataset it should load data from.\n",
    "\n",
    "The `batch_size` arg also does not require any explanation.\n",
    "\n",
    "The `shuffle` arg specifies whether the data in the dataset should be shuffled before sampling. If it is set to `True`, the data in the sataset will be shuffled before sampling. Besides, the loader will by default use a [RandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler) for loading data. It it is set to `False`, the data in the dataset will not be shuffled before sampling. And the loader will by default use a [SequentialSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler) for loading data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf434a2f-35dd-4bae-9d90-a8821559ea00",
   "metadata": {},
   "source": [
    "### 11.3 the `collate_fn` Argument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9af3a1-5658-4302-9e8e-43490fac5958",
   "metadata": {},
   "source": [
    "Now, there is only one arg left: `collate_fn`. According to the [doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), it is a `Callable` (function) that *merges a list of samples to form a mini-batch of Tensor(s).* In short, two sets of operations are to be done in the `collate_fn` function:\n",
    "1. Processing the data. For the texts, it includes: tokenization, converting tokens to ids, **padding**. For the labels, it includes: textual classes to ids.\n",
    "2. Converting the texts into a tensor, and converting the labels into another tensor. Wrap the two tensors into a batch.\n",
    "\n",
    "It should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "341d53db-cf30-4dd6-820d-81be63357790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(samples, **kwargs):\n",
    "    \"\"\"Collate function for training / validation / testing.\n",
    "\n",
    "    :param samples: A list of (text, label).\n",
    "    :param kwargs: Other needed args.\n",
    "    :return: Collated batch.\n",
    "    \"\"\"\n",
    "    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c293111-3c22-4af1-a092-2c33b2c45789",
   "metadata": {},
   "source": [
    "#### 11.3.1 Input of `collate_fn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4b44b-a4f2-4c38-b584-c5462f5f7b74",
   "metadata": {},
   "source": [
    "To make everything intuitive, let's first see what the input of the `collate_fn` looks like (assume that the batch size is 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26d6af54-c8e8-4ed8-a25d-9865d98bb887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PeopleSoft accepts latest Oracle offer Oracle Corp. #39;s 18-month struggle to gain control of PeopleSoft Inc. drew to a close this morning when PeopleSoft #39;s board agreed to a new \\\\$10.3 billion takeover offer.',\n",
       "  'Business'),\n",
       " ('The wife, a stranger, the bully and a bullet Marlene Brookes is a shirt-presser, a quiet, conscientious woman who has looked after Bay Street #39;s demanding clients for more than eight years.',\n",
       "  'World'),\n",
       " ('44 believed North Koreans clamber over fence into Canadian Embassy BEIJING (CP) - China said Thursday it wants the Canadian Embassy to hand over 44 people thought to be North Korean asylum-seekers who climbed over a spiked fence onto embassy grounds.',\n",
       "  'World'),\n",
       " (\"Carter to Miss Two Games to Fight Lawsuit (AP) AP - Vince Carter will miss the Toronto Raptors' next two preseason games while he fights a lawsuit from a former agent.\",\n",
       "  'Sports'),\n",
       " ('More attackers targeting e-commerce and Web apps, says Symantec The total number of virus attacks are down, but malicious codemeisters are getting faster, more sophisticated, and they #39;re beginning to target e-commerce concerns and small businesses.',\n",
       "  'Sci/Tech'),\n",
       " (\"UK's EMI Says to Face Music Industry Probe in U.S.  LONDON/NEW YORK (Reuters) - EMI Group PLC, the world's  third-largest music company, on Friday said it and other music  companies faced a New York probe into how music companies  influence what songs are played on the radio.\",\n",
       "  'Business'),\n",
       " ('D-Backs get Glaus PHOENIX The Arizona Diamondbacks have worked out a four-year deal with third baseman Troy Glaus. The 2002 World Series MVP hit just .251 with 18 homers and 42 RBI #39;s in 58 games this year, missing much of the season due to shoulder surgery.',\n",
       "  'Sports'),\n",
       " ('Karzai declared winner of Afghan election KABUL, Afghanistan -- Hamid Karzai was officially declared Afghanistan #39;s first-ever popularly elected president today after a weeks-long fraud probe found no reason to overturn his landslide victory.',\n",
       "  'World')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[  \n",
    "    ('PeopleSoft accepts latest Oracle offer Oracle Corp. #39;s 18-month struggle to gain control of PeopleSoft Inc. drew to a close this morning when PeopleSoft #39;s board agreed to a new \\\\$10.3 billion takeover offer.', 'Business'),  \n",
    "    ('The wife, a stranger, the bully and a bullet Marlene Brookes is a shirt-presser, a quiet, conscientious woman who has looked after Bay Street #39;s demanding clients for more than eight years.', 'World'),  \n",
    "    ('44 believed North Koreans clamber over fence into Canadian Embassy BEIJING (CP) - China said Thursday it wants the Canadian Embassy to hand over 44 people thought to be North Korean asylum-seekers who climbed over a spiked fence onto embassy grounds.', 'World'),  \n",
    "    (\"Carter to Miss Two Games to Fight Lawsuit (AP) AP - Vince Carter will miss the Toronto Raptors' next two preseason games while he fights a lawsuit from a former agent.\", 'Sports'),   \n",
    "    ('More attackers targeting e-commerce and Web apps, says Symantec The total number of virus attacks are down, but malicious codemeisters are getting faster, more sophisticated, and they #39;re beginning to target e-commerce concerns and small businesses.', 'Sci/Tech'),   \n",
    "    (\"UK's EMI Says to Face Music Industry Probe in U.S.  LONDON/NEW YORK (Reuters) - EMI Group PLC, the world's  third-largest music company, on Friday said it and other music  companies faced a New York probe into how music companies  influence what songs are played on the radio.\", 'Business'),   \n",
    "    ('D-Backs get Glaus PHOENIX The Arizona Diamondbacks have worked out a four-year deal with third baseman Troy Glaus. The 2002 World Series MVP hit just .251 with 18 homers and 42 RBI #39;s in 58 games this year, missing much of the season due to shoulder surgery.', 'Sports'),   \n",
    "    ('Karzai declared winner of Afghan election KABUL, Afghanistan -- Hamid Karzai was officially declared Afghanistan #39;s first-ever popularly elected president today after a weeks-long fraud probe found no reason to overturn his landslide victory.', 'World')  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5348b1-bc5d-48f3-b824-cc21978aede0",
   "metadata": {},
   "source": [
    "We see that the input is a list, each element of the list is a tuple of (text, label).\n",
    "\n",
    "Here's how the input is generated: First the data loader samples `batch_size` samples from the `dataset`. if `shuffle` is specified to `True`, the data are sampled randomly with the `RandomSampler`, and if `shuffle` is specified to `False`, the data are sampled sequentially with the `SequentialSampler`.\n",
    "\n",
    "When sampling data from the dataset, the `__getitem__` method of the `dataset` will ba called. As a reminder, the following code is the `__getitem__` method we implemented earlier. This method returns a tuple of (text, label), which is exactly the form of elements in the list shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5e46ed7d-a2b4-442c-a99d-dbf07029b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getitem__(self, idx: int) -> Tuple[str, str]:\n",
    "    text = self.texts[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d8070a-a48a-4f9b-b838-54441e8c485f",
   "metadata": {},
   "source": [
    "Our task now is to make a batch. It contains several steps, as described in the following sections. To make everything clear, let assume that the samples are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d96b9c8-3cc3-47b0-ab01-28d292201e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [  \n",
    "    ('PeopleSoft accepts latest Oracle offer Oracle Corp. #39;s 18-month struggle to gain control of PeopleSoft Inc. drew to a close this morning when PeopleSoft #39;s board agreed to a new \\\\$10.3 billion takeover offer.', 'Business'),  \n",
    "    ('The wife, a stranger, the bully and a bullet Marlene Brookes is a shirt-presser, a quiet, conscientious woman who has looked after Bay Street #39;s demanding clients for more than eight years.', 'World'),  \n",
    "    ('44 believed North Koreans clamber over fence into Canadian Embassy BEIJING (CP) - China said Thursday it wants the Canadian Embassy to hand over 44 people thought to be North Korean asylum-seekers who climbed over a spiked fence onto embassy grounds.', 'World'),  \n",
    "    (\"Carter to Miss Two Games to Fight Lawsuit (AP) AP - Vince Carter will miss the Toronto Raptors' next two preseason games while he fights a lawsuit from a former agent.\", 'Sports'),   \n",
    "    ('More attackers targeting e-commerce and Web apps, says Symantec The total number of virus attacks are down, but malicious codemeisters are getting faster, more sophisticated, and they #39;re beginning to target e-commerce concerns and small businesses.', 'Sci/Tech'),   \n",
    "    (\"UK's EMI Says to Face Music Industry Probe in U.S.  LONDON/NEW YORK (Reuters) - EMI Group PLC, the world's  third-largest music company, on Friday said it and other music  companies faced a New York probe into how music companies  influence what songs are played on the radio.\", 'Business'),   \n",
    "    ('D-Backs get Glaus PHOENIX The Arizona Diamondbacks have worked out a four-year deal with third baseman Troy Glaus. The 2002 World Series MVP hit just .251 with 18 homers and 42 RBI #39;s in 58 games this year, missing much of the season due to shoulder surgery.', 'Sports'),   \n",
    "    ('Karzai declared winner of Afghan election KABUL, Afghanistan -- Hamid Karzai was officially declared Afghanistan #39;s first-ever popularly elected president today after a weeks-long fraud probe found no reason to overturn his landslide victory.', 'World')  \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b8052-a267-4377-a556-4f7c0ef70606",
   "metadata": {},
   "source": [
    "#### 11.3.2 Obtain Texts and Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f417cb-070c-47cf-9954-083e7532e81d",
   "metadata": {},
   "source": [
    "First, we should put all texts into a list, and all labels into another. Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71114b67-a053-4bfb-b2f7-f3dfee21154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      "('PeopleSoft accepts latest Oracle offer Oracle Corp. #39;s 18-month struggle to gain control of PeopleSoft Inc. drew to a close this morning when PeopleSoft #39;s board agreed to a new \\\\$10.3 billion takeover offer.', 'The wife, a stranger, the bully and a bullet Marlene Brookes is a shirt-presser, a quiet, conscientious woman who has looked after Bay Street #39;s demanding clients for more than eight years.', '44 believed North Koreans clamber over fence into Canadian Embassy BEIJING (CP) - China said Thursday it wants the Canadian Embassy to hand over 44 people thought to be North Korean asylum-seekers who climbed over a spiked fence onto embassy grounds.', \"Carter to Miss Two Games to Fight Lawsuit (AP) AP - Vince Carter will miss the Toronto Raptors' next two preseason games while he fights a lawsuit from a former agent.\", 'More attackers targeting e-commerce and Web apps, says Symantec The total number of virus attacks are down, but malicious codemeisters are getting faster, more sophisticated, and they #39;re beginning to target e-commerce concerns and small businesses.', \"UK's EMI Says to Face Music Industry Probe in U.S.  LONDON/NEW YORK (Reuters) - EMI Group PLC, the world's  third-largest music company, on Friday said it and other music  companies faced a New York probe into how music companies  influence what songs are played on the radio.\", 'D-Backs get Glaus PHOENIX The Arizona Diamondbacks have worked out a four-year deal with third baseman Troy Glaus. The 2002 World Series MVP hit just .251 with 18 homers and 42 RBI #39;s in 58 games this year, missing much of the season due to shoulder surgery.', 'Karzai declared winner of Afghan election KABUL, Afghanistan -- Hamid Karzai was officially declared Afghanistan #39;s first-ever popularly elected president today after a weeks-long fraud probe found no reason to overturn his landslide victory.')\n",
      "\n",
      "labels:\n",
      "('Business', 'World', 'World', 'Sports', 'Sci/Tech', 'Business', 'Sports', 'World')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, labels = list(zip(*samples))\n",
    "\n",
    "# Check.\n",
    "print(f\"texts:\\n{texts}\\n\")\n",
    "print(f\"labels:\\n{labels}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96750bb7-e485-4393-89a7-1690b35e488b",
   "metadata": {},
   "source": [
    "#### 11.3.3 Process the Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54bef4-5b13-418e-bd36-0dfe9690efff",
   "metadata": {},
   "source": [
    "Then, we should convert each text into token ids with the tokenizer, vocab we constructed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8854a7b-81bc-4961-b0e4-8cf093d951f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[367, 5233, 315, 311, 374, 311, 87, 1, 12, 9, 8489, 2581, 4, 1167, 649, 6, 367, 64, 1, 2844, 4, 5, 419, 52, 665, 90, 367, 12, 9, 515, 300, 4, 5, 23, 1729, 1, 220, 139, 696, 374, 1], [2, 1995, 3, 5, 12148, 3, 2, 15809, 8, 5, 9416, 28566, 0, 21, 5, 0, 3, 5, 3682, 3, 0, 1126, 75, 28, 2029, 34, 876, 375, 12, 9, 3258, 3590, 11, 47, 72, 631, 97, 1], [4219, 2212, 267, 6284, 0, 38, 7727, 66, 364, 1422, 848, 13, 1007, 14, 15, 118, 26, 60, 25, 716, 2, 364, 1422, 4, 1196, 38, 4219, 102, 1400, 4, 37, 267, 1259, 25237, 75, 2893, 38, 5, 14638, 7727, 2568, 1422, 6458, 1], [2432, 4, 1015, 49, 217, 4, 557, 974, 13, 31, 14, 31, 15, 4913, 2432, 33, 1015, 2, 733, 3031, 16, 109, 49, 2749, 217, 224, 48, 4352, 5, 974, 29, 5, 140, 2049, 1], [47, 5308, 2973, 5393, 8, 226, 5853, 3, 84, 2552, 2, 1352, 436, 6, 1421, 400, 42, 134, 3, 45, 3889, 0, 42, 883, 1971, 3, 47, 6616, 3, 8, 67, 12, 999, 1623, 4, 776, 5393, 770, 8, 653, 971, 1], [390, 16, 9, 7995, 84, 4, 449, 211, 241, 765, 7, 51, 1, 9, 1, 22892, 73, 13, 27, 14, 15, 7995, 96, 1632, 3, 2, 50, 16, 9, 4790, 211, 54, 3, 10, 65, 26, 25, 8, 157, 211, 215, 2677, 5, 23, 73, 765, 66, 359, 211, 215, 3478, 183, 2448, 42, 1241, 10, 2, 699, 1], [14439, 223, 8425, 3033, 2, 1575, 3622, 39, 3828, 59, 5, 3004, 120, 18, 190, 4092, 4527, 8425, 1, 2, 1921, 50, 233, 4208, 230, 176, 1, 23162, 18, 654, 4385, 8, 4118, 6447, 12, 9, 7, 7085, 217, 52, 71, 3, 921, 387, 6, 2, 117, 549, 4, 3180, 1455, 1], [1288, 2001, 1292, 6, 718, 244, 1367, 3, 669, 53, 2160, 1288, 35, 2429, 2001, 669, 12, 9, 5806, 12472, 3339, 77, 100, 34, 5, 0, 909, 765, 324, 78, 2223, 4, 5942, 32, 5609, 146, 1]]\n"
     ]
    }
   ],
   "source": [
    "texts = list(map(\n",
    "    lambda text: vocab(tokenizer(text)),\n",
    "    texts\n",
    "))\n",
    "\n",
    "# Check.\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b585f45-3158-49f2-bcf1-bfb36404e8ba",
   "metadata": {},
   "source": [
    "Done! Now let's convert them into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef9b9afe-9188-4fb2-8a39-a05d8b5fe38f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 41 at dim 1 (got 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 41 at dim 1 (got 38)"
     ]
    }
   ],
   "source": [
    "texts = torch.tensor(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8722e2a-0a07-493c-b156-897b1f00b8ff",
   "metadata": {},
   "source": [
    "Oops! Something went wrong.\n",
    "\n",
    "The reason is that the elements in `texts` have inconsistent lengths. Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f8bd9ee-854e-4626-9384-15acfbf885cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "38\n",
      "44\n",
      "35\n",
      "42\n",
      "59\n",
      "52\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "for token_ids in texts:\n",
    "    print(len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd3dd83-0400-44fd-b5f2-6b588f2899bd",
   "metadata": {},
   "source": [
    "However, to convert `texts` into a tensor, each text should has the same length. Therefore, we should handle the length problem by **padding**. A natural way for padding is to pad each text into the max text length in the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6055b319-5a0a-4304-937f-b869692277b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len: 59\n",
      "[[367, 5233, 315, 311, 374, 311, 87, 1, 12, 9, 8489, 2581, 4, 1167, 649, 6, 367, 64, 1, 2844, 4, 5, 419, 52, 665, 90, 367, 12, 9, 515, 300, 4, 5, 23, 1729, 1, 220, 139, 696, 374, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1995, 3, 5, 12148, 3, 2, 15809, 8, 5, 9416, 28566, 0, 21, 5, 0, 3, 5, 3682, 3, 0, 1126, 75, 28, 2029, 34, 876, 375, 12, 9, 3258, 3590, 11, 47, 72, 631, 97, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4219, 2212, 267, 6284, 0, 38, 7727, 66, 364, 1422, 848, 13, 1007, 14, 15, 118, 26, 60, 25, 716, 2, 364, 1422, 4, 1196, 38, 4219, 102, 1400, 4, 37, 267, 1259, 25237, 75, 2893, 38, 5, 14638, 7727, 2568, 1422, 6458, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2432, 4, 1015, 49, 217, 4, 557, 974, 13, 31, 14, 31, 15, 4913, 2432, 33, 1015, 2, 733, 3031, 16, 109, 49, 2749, 217, 224, 48, 4352, 5, 974, 29, 5, 140, 2049, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [47, 5308, 2973, 5393, 8, 226, 5853, 3, 84, 2552, 2, 1352, 436, 6, 1421, 400, 42, 134, 3, 45, 3889, 0, 42, 883, 1971, 3, 47, 6616, 3, 8, 67, 12, 999, 1623, 4, 776, 5393, 770, 8, 653, 971, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [390, 16, 9, 7995, 84, 4, 449, 211, 241, 765, 7, 51, 1, 9, 1, 22892, 73, 13, 27, 14, 15, 7995, 96, 1632, 3, 2, 50, 16, 9, 4790, 211, 54, 3, 10, 65, 26, 25, 8, 157, 211, 215, 2677, 5, 23, 73, 765, 66, 359, 211, 215, 3478, 183, 2448, 42, 1241, 10, 2, 699, 1], [14439, 223, 8425, 3033, 2, 1575, 3622, 39, 3828, 59, 5, 3004, 120, 18, 190, 4092, 4527, 8425, 1, 2, 1921, 50, 233, 4208, 230, 176, 1, 23162, 18, 654, 4385, 8, 4118, 6447, 12, 9, 7, 7085, 217, 52, 71, 3, 921, 387, 6, 2, 117, 549, 4, 3180, 1455, 1, 0, 0, 0, 0, 0, 0, 0], [1288, 2001, 1292, 6, 718, 244, 1367, 3, 669, 53, 2160, 1288, 35, 2429, 2001, 669, 12, 9, 5806, 12472, 3339, 77, 100, 34, 5, 0, 909, 765, 324, 78, 2223, 4, 5942, 32, 5609, 146, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "# Find the max text length.\n",
    "max_len = 0\n",
    "for token_ids in texts:\n",
    "    max_len = max(len(token_ids), max_len)\n",
    "print(f\"max len: {max_len}\")\n",
    "\n",
    "# Padding:\n",
    "default_pad_val = 0\n",
    "texts = list(map(\n",
    "    lambda token_ids: token_ids + ([default_pad_val] * (max_len - len(token_ids))),\n",
    "    texts\n",
    "))\n",
    "# Check.\n",
    "print(texts)\n",
    "for token_ids in texts:\n",
    "    print(len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c11e2-8682-449e-b6b1-d8a198a179f6",
   "metadata": {},
   "source": [
    "A better way for padding is to pad each text into a `max_len`, instead of the max text length in the samples. The reason is that sometimes the max text length in the samples is very large, so other texts in the samples will be padded to this very large max text length. However, if the text length is too long, the RNN may fail to capture long distance textual info. Let's modify the code above by padding to a `max_len` instead of the max text length in the samples.\n",
    "\n",
    "Note that in this case, some texts in the samples will have lengths exceeding the specified `max_len`, which should be **truncated** instead of padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a4bb486d-b62c-4f7e-ad02-81b446fd9a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[367, 5233, 315, 311, 374, 311, 87, 1, 12, 9, 8489, 2581, 4, 1167, 649, 6, 367, 64, 1, 2844, 4, 5, 419, 52, 665], [2, 1995, 3, 5, 12148, 3, 2, 15809, 8, 5, 9416, 28566, 0, 21, 5, 0, 3, 5, 3682, 3, 0, 1126, 75, 28, 2029], [4219, 2212, 267, 6284, 0, 38, 7727, 66, 364, 1422, 848, 13, 1007, 14, 15, 118, 26, 60, 25, 716, 2, 364, 1422, 4, 1196], [2432, 4, 1015, 49, 217, 4, 557, 974, 13, 31, 14, 31, 15, 4913, 2432, 33, 1015, 2, 733, 3031, 16, 109, 49, 2749, 217], [47, 5308, 2973, 5393, 8, 226, 5853, 3, 84, 2552, 2, 1352, 436, 6, 1421, 400, 42, 134, 3, 45, 3889, 0, 42, 883, 1971], [390, 16, 9, 7995, 84, 4, 449, 211, 241, 765, 7, 51, 1, 9, 1, 22892, 73, 13, 27, 14, 15, 7995, 96, 1632, 3], [14439, 223, 8425, 3033, 2, 1575, 3622, 39, 3828, 59, 5, 3004, 120, 18, 190, 4092, 4527, 8425, 1, 2, 1921, 50, 233, 4208, 230], [1288, 2001, 1292, 6, 718, 244, 1367, 3, 669, 53, 2160, 1288, 35, 2429, 2001, 669, 12, 9, 5806, 12472, 3339, 77, 100, 34, 5]]\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "max_len = 25\n",
    "\n",
    "# Padding:\n",
    "default_pad_val = 0\n",
    "texts = list(map(\n",
    "    lambda token_ids: (\n",
    "        token_ids + ([default_pad_val] * (max_len - len(token_ids)))  # Less than max_len: padding.\n",
    "        if len(token_ids) < max_len  \n",
    "        else token_ids[:max_len]  # Greater than max_len: truncation.\n",
    "    ),\n",
    "    texts\n",
    "))\n",
    "# Check.\n",
    "print(texts)\n",
    "for token_ids in texts:\n",
    "    print(len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d4d80c-8230-4fd2-b7cc-2f7e4b95e502",
   "metadata": {},
   "source": [
    "After padding, let's convert `texts` into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c7a3c8cf-0fba-460c-b505-e7f828ffe29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  367,  5233,   315,   311,   374,   311,    87,     1,    12,     9,\n",
       "          8489,  2581,     4,  1167,   649,     6,   367,    64,     1,  2844,\n",
       "             4,     5,   419,    52,   665],\n",
       "        [    2,  1995,     3,     5, 12148,     3,     2, 15809,     8,     5,\n",
       "          9416, 28566,     0,    21,     5,     0,     3,     5,  3682,     3,\n",
       "             0,  1126,    75,    28,  2029],\n",
       "        [ 4219,  2212,   267,  6284,     0,    38,  7727,    66,   364,  1422,\n",
       "           848,    13,  1007,    14,    15,   118,    26,    60,    25,   716,\n",
       "             2,   364,  1422,     4,  1196],\n",
       "        [ 2432,     4,  1015,    49,   217,     4,   557,   974,    13,    31,\n",
       "            14,    31,    15,  4913,  2432,    33,  1015,     2,   733,  3031,\n",
       "            16,   109,    49,  2749,   217],\n",
       "        [   47,  5308,  2973,  5393,     8,   226,  5853,     3,    84,  2552,\n",
       "             2,  1352,   436,     6,  1421,   400,    42,   134,     3,    45,\n",
       "          3889,     0,    42,   883,  1971],\n",
       "        [  390,    16,     9,  7995,    84,     4,   449,   211,   241,   765,\n",
       "             7,    51,     1,     9,     1, 22892,    73,    13,    27,    14,\n",
       "            15,  7995,    96,  1632,     3],\n",
       "        [14439,   223,  8425,  3033,     2,  1575,  3622,    39,  3828,    59,\n",
       "             5,  3004,   120,    18,   190,  4092,  4527,  8425,     1,     2,\n",
       "          1921,    50,   233,  4208,   230],\n",
       "        [ 1288,  2001,  1292,     6,   718,   244,  1367,     3,   669,    53,\n",
       "          2160,  1288,    35,  2429,  2001,   669,    12,     9,  5806, 12472,\n",
       "          3339,    77,   100,    34,     5]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = torch.tensor(texts)\n",
    "\n",
    "# Check.\n",
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421106f2-6f7b-4d69-b3d1-3e68c8e1e203",
   "metadata": {},
   "source": [
    "Since all texts in the samples have lengths greater than the `max_len` (25), they are truncated instead of padded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7a0b19-dcef-40a2-9cd4-c6e6c5f6d725",
   "metadata": {},
   "source": [
    "#### 11.3.4 Process the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab32672-b7ef-4a82-b1dd-1acf664fa078",
   "metadata": {},
   "source": [
    "The labels should be (1) mapped from textual classes to ids and (2) converted into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "addb2899-5720-4be3-867c-8ecfb91821e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 3, 3, 0, 2, 1, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "labels = torch.tensor(list(map(\n",
    "    lambda label: class_index_mapping[label],\n",
    "    labels\n",
    ")))\n",
    "\n",
    "# Check.\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf573344-62ff-430b-a253-7541e6b916ec",
   "metadata": {},
   "source": [
    "#### 11.3.5 Make a Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792b57f-edca-419c-8fe9-afb509d81b59",
   "metadata": {},
   "source": [
    "Very simple. Just wrap the `texts` and `labels` into a Python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bec78195-9258-4272-a664-d1e1378cd635",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    \"texts\": texts,\n",
    "    \"labels\": labels,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1010c-46b2-4c27-b824-7bf46f023b9b",
   "metadata": {},
   "source": [
    "### 11.4 Complete Implementation for `collate_fn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42dc7a77-9154-466a-afa3-bf1e4295eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(token_indices: List[int], max_len: int, default_pad_val: int = 0) -> List[int]:\n",
    "    \"\"\"Pad the given token indices.\n",
    "\n",
    "    :param token_indices: Indices of the tokens.\n",
    "    :param max_len: Max sentence length.\n",
    "    :param default_pad_val: Default padding value.\n",
    "    :return: Padded indices.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(token_indices) < max_len:\n",
    "        return token_indices + ([default_pad_val] * (max_len - len(token_indices)))\n",
    "    else:\n",
    "        return token_indices[:max_len]\n",
    "    \n",
    "def collate_func(\n",
    "        samples: List[Tuple[str, str]],\n",
    "        vocab: torchtext.vocab.Vocab, tokenizer: Callable, max_len: int,\n",
    "        class_index_mapping: Dict[str, int]\n",
    ") -> Dict[str, torch.tensor]:\n",
    "    \"\"\"Collate function for training / validation / testing.\n",
    "\n",
    "    :param samples: A list of (text, label).\n",
    "    :param vocab: Vocabulary.\n",
    "    :param tokenizer: Tokenizer.\n",
    "    :param max_len: Max len of the sentences.\n",
    "    :param class_index_mapping: Mapping from label string to label index.\n",
    "    :return: Collated batch.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = list(zip(*samples))\n",
    "\n",
    "    texts = torch.tensor(list(map(\n",
    "        lambda text: pad(\n",
    "            vocab(tokenizer(text)),\n",
    "            max_len=max_len\n",
    "        ),\n",
    "        texts\n",
    "    )))\n",
    "\n",
    "    labels = torch.tensor(list(map(\n",
    "        lambda label: class_index_mapping[label],\n",
    "        labels\n",
    "    )))\n",
    "\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbedf670-92cf-4506-b71d-f876e51f35e3",
   "metadata": {},
   "source": [
    "Note that in the implementation above, the batch is in the form of a `dict`. But actually, **you can return the `texts` and `labels` directly in `collate_fn` with `return texts, labels`**. The return value format of `collate_fn` is not restricted!\n",
    "\n",
    "However, sometimes you have to return many things from `collate_fn`, so returning a `dict` is more preferable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a912a80-a0bb-4351-b838-e71e4922ea77",
   "metadata": {},
   "source": [
    "### 11.5 Creating Data Loaders for the Training/Development/Test Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1353c7-b4e5-47ca-bf9e-556c6616d609",
   "metadata": {},
   "source": [
    "Now we've gone through the four args of `DataLoader`. Let's create a data loader for each dataset.\n",
    "\n",
    "See how the **arguments** are passed to `collate_fn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ab93517-be5c-4f42-906e-ddd95b6f6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_len = 25\n",
    "train_bz = 64\n",
    "eval_bz = 64\n",
    "\n",
    "collate_fn = lambda samples: collate_func(\n",
    "    samples=samples,\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=vocab,\n",
    "    max_len=max_len,\n",
    "    class_index_mapping=class_index_mapping,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_bz,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=eval_bz,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=eval_bz,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97f665-b324-47f0-86dd-52127ff1f2f7",
   "metadata": {},
   "source": [
    "## 12. Training, Validation, Testing and Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c41d4-b886-4e92-986e-5d3201757bab",
   "metadata": {},
   "source": [
    "### 12.1 Goal of this Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3cb3d-d7da-4526-972e-b932227b053d",
   "metadata": {},
   "source": [
    "Now everything is ready for training. In this section, we implement the code for training, validation, testing and prediction. Pseudo code:\n",
    "\n",
    "```python\n",
    "best_valid_loss = float(\"inf\")\n",
    "best_model = None\n",
    "for i in range(num_epochs):\n",
    "    train(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader\n",
    "    )\n",
    "\n",
    "    valid_loss = valid(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        dev_loader=dev_loader\n",
    "    )\n",
    "    # Update the best model.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "test(\n",
    "    model=best_model,\n",
    "    test_loader=test_loader,\n",
    "    class_index_mapping=class_index_mapping\n",
    ")\n",
    "\n",
    "predict(\n",
    "    model=best_model,\n",
    "    text=\"Apple Tops in Customer Satisfaction \\\"Dell comes in a close second, while Gateway shows improvement, study says.\\\"\",\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=vocab,\n",
    "    class_index_mapping=class_index_mapping,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28fff7-c031-4887-8f4b-f222ce9e0ee8",
   "metadata": {},
   "source": [
    "### 12.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e155b-07b5-4bf3-bc85-cf52b12fb687",
   "metadata": {},
   "source": [
    "See the code below.\n",
    "\n",
    "+ Notes of **`model.train()`**: According to the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=train#torch.nn.Module.train), it *sets the module in **training mode**. This has any effect **only on certain modules**. See **documentations** of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.* In other words, it **may not** by necessary. However, adding this line of code is a good practice, as it will have effect in certain situations.\n",
    "\n",
    "+ `batch` is what is returned by `collate_fn`.\n",
    "\n",
    "+ `model(texts)` calls the `forward` method of the model. In other words, it is equivalent to calling `model.forward(texts)`.\n",
    "\n",
    "+ The return value of `model(texts)`, i.e., `predictions`, is a list of logits with size (`batch_size`, `class_num`). When you print it, you will find it looks like something above (when the batch size is 8). Each line corresponds to the prediction probabilities of a training example. For example, the first line \\[2.2923e+00, -4.2605e+00,  1.4412e+00, -9.4200e-01\\] is the prediction probabilities of a certain training example. It means: the model thinks there is a 2.2923 prob that the text of the training example belongs to the first class (i.e., `class_index_mapping[0]`) , a -4.2605 prob that the text of the training example belongs to the second class (i.e., `class_index_mapping[1]`), ... . Since there are four classes for text classification in this tutorial, each line has four values, correspinding to the prediction prob of the four classes.\n",
    "\n",
    "```python\n",
    "tensor([[ 2.2923e+00, -4.2605e+00,  1.4412e+00, -9.4200e-01],\n",
    "        [ 1.9423e+00, -3.3877e+00,  2.1381e+00, -2.2040e+00],\n",
    "        [ 5.5345e-01, -2.1561e+00,  3.7267e-02,  9.9266e-01],\n",
    "        [-2.1681e+00,  4.1377e+00, -1.4757e+00, -3.7116e-01],\n",
    "        [ 2.2583e+00, -3.7365e+00,  5.5374e-01,  3.8267e-02],\n",
    "        [ 7.2302e-01, -2.5620e+00,  1.5886e+00, -1.2569e+00],\n",
    "        [-2.3920e+00,  3.1881e+00, -1.3345e+00, -1.8628e-01],\n",
    "        [-9.9738e-01, -1.2172e-01, -1.2999e+00,  2.8284e+00], grad_fn=<AddmmBackward0>)\n",
    "```\n",
    "\n",
    "+ Note that the sizes of `predictions` and `labels` in `loss = criterion(predictions, labels)` are different. `predictions`: (`batch_size`, `class_num`), `labels`: (`batch_size`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23042d98-441a-408c-91bd-3ae0ed202adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If gpu (cuda) is available, use gpu for training. Otherwise use cpu.\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        texts = batch[\"texts\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        predictions = model(texts)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.tensor(losses).mean()\n",
    "    print(f\"Train Loss : {train_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844fa4e-5019-49ee-a32a-584c82607701",
   "metadata": {},
   "source": [
    "### 12.3 Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523be6c-c543-401b-b03b-6aaed6feb5d8",
   "metadata": {},
   "source": [
    "The code for validation is similar to that for training.\n",
    "\n",
    "+ `torch.no_grad()`: For stopping tracking computations. When `model(texts)` is executed without `torch.no_grad()`, a lot of things are stored to do backpropogation, which requires large cpu/gpu storage. When validating, we don't need them. See more details [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html).\n",
    "\n",
    "+ `predictions.argmax(dim=-1)` is for getting the most likely label id (with the greated prob) for each text in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03448b11-3de0-417f-8dc4-d6d706e9384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, criterion, dev_loader):\n",
    "    \"\"\"Validate the model performance.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []  # Store all labels in the dev data.\n",
    "    all_predictions = []  # Store all predictions.\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            texts = batch[\"texts\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)  # labels here: labels in a batch.\n",
    "\n",
    "            predictions = model(texts)  # predictions here: predictions in a batch.\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predictions.argmax(dim=-1))\n",
    "\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    valid_loss = torch.tensor(losses).mean()\n",
    "    valid_acc = accuracy_score(\n",
    "        y_true=all_labels.detach().cpu().numpy(),\n",
    "        y_pred=all_predictions.detach().cpu().numpy()\n",
    "    )\n",
    "    print(f\"Valid Loss : {valid_loss:.3f}\")\n",
    "    print(f\"Valid Acc  : {valid_acc:.3f}\")\n",
    "\n",
    "    return valid_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35c1b0-3fe8-43bf-8ca8-bd7d8c01173e",
   "metadata": {},
   "source": [
    "### 12.4 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1b528399-927d-4315-ba31-d939a73e2872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, class_index_mapping):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            texts = batch[\"texts\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            predictions = model(texts)\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "    all_labels = all_labels.detach().cpu().numpy()\n",
    "    all_predictions = F.softmax(all_predictions, dim=-1).argmax(dim=-1).detach().cpu().numpy()\n",
    "\n",
    "    test_acc = accuracy_score(\n",
    "        y_true=all_labels,\n",
    "        y_pred=all_predictions\n",
    "    )\n",
    "    print(f\"Test Acc   : {test_acc:.3f}\")\n",
    "\n",
    "    print(\"\\nClassification Report : \")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=class_index_mapping.keys()))\n",
    "\n",
    "    print(\"\\nConfusion Matrix : \")\n",
    "    print(confusion_matrix(all_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad884b-4ff9-4495-8257-4c21ab360f91",
   "metadata": {},
   "source": [
    "### 12.5 Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715623aa-0352-4b0c-a2b6-270f9506a1aa",
   "metadata": {},
   "source": [
    "After training/validation/testing, we can use it to predict the news category of the given news.\n",
    "\n",
    "Given a piece of news (text), we first need to tokenize it and convert the tokens to token ids. Then, we should wrap it into a tensor with `torch.tensor`. Note that we put it into a list because the input dimension required by the model is (`batch_size`, text_len). If we put it into a list, the resulting tensor will be of (1, text_len), where the 1 here is the batch size. But if not, the resulting tensor will have a dimension of (text_len).\n",
    "\n",
    "After processing the given text, we feed it to the model and get the `predictions`. Note that `predictions` is of (`batch_size`, `class_num`), as mentioned earlier. Here the batch size is 1, so it is of (1, `class_num`).\n",
    "\n",
    "We then obtain the most likely label id `prediction_index` with `argmax`. After that, we convert the label id to the original textual class `prediction_class` with the reversed version of `class_index_mapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2539697f-0903-402b-b826-88b546b2956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text, tokenizer, vocab, class_index_mapping):\n",
    "    \"\"\"Predict the label of the given text.\"\"\"\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = vocab(tokens)\n",
    "    texts = torch.tensor([token_ids])\n",
    "    with torch.no_grad():\n",
    "        predictions = model(texts)\n",
    "\n",
    "    prediction_index = predictions[0].argmax(dim=0).item()\n",
    "    prediction_class = {\n",
    "        index: class_\n",
    "        for class_, index in class_index_mapping.items()\n",
    "    }[prediction_index]\n",
    "\n",
    "    print(f\"\\ntext: {text}\")\n",
    "    print(f\"prediction: {prediction_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63aa85-a1eb-4f4c-94a0-137ca07d5633",
   "metadata": {},
   "source": [
    "## 13. Complete Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f186cf71-cf50-463a-9f3e-3222e84d09dc",
   "metadata": {},
   "source": [
    "Now let's put all code together and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e3ef36b-319e-4635-95b4-891ce3f603b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:41<00:00, 43.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.862\n",
      "Valid Loss : 0.586\n",
      "Valid Acc  : 0.796\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:29<00:00, 60.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.479\n",
      "Valid Loss : 0.461\n",
      "Valid Acc  : 0.850\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:39<00:00, 45.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.383\n",
      "Valid Loss : 0.427\n",
      "Valid Acc  : 0.866\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:54<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.329\n",
      "Valid Loss : 0.410\n",
      "Valid Acc  : 0.866\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:54<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.290\n",
      "Valid Loss : 0.413\n",
      "Valid Acc  : 0.864\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:54<00:00, 32.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.261\n",
      "Valid Loss : 0.410\n",
      "Valid Acc  : 0.875\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:55<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.238\n",
      "Valid Loss : 0.411\n",
      "Valid Acc  : 0.878\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:53<00:00, 33.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.215\n",
      "Valid Loss : 0.418\n",
      "Valid Acc  : 0.878\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:52<00:00, 33.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.198\n",
      "Valid Loss : 0.414\n",
      "Valid Acc  : 0.878\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1782/1782 [00:54<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss : 0.178\n",
      "Valid Loss : 0.414\n",
      "Valid Acc  : 0.878\n",
      "Test Acc   : 0.880\n",
      "\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Sports       0.94      0.96      0.95      1900\n",
      "    Business       0.84      0.84      0.84      1900\n",
      "    Sci/Tech       0.81      0.89      0.85      1900\n",
      "       World       0.94      0.83      0.88      1900\n",
      "\n",
      "    accuracy                           0.88      7600\n",
      "   macro avg       0.88      0.88      0.88      7600\n",
      "weighted avg       0.88      0.88      0.88      7600\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      "[[1816   11   47   26]\n",
      " [  14 1600  242   44]\n",
      " [  36  141 1686   37]\n",
      " [  61  153  101 1585]]\n",
      "\n",
      "text: Apple Tops in Customer Satisfaction \"Dell comes in a close second, while Gateway shows improvement, study says.\"\n",
      "prediction: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import csv\n",
    "import torch\n",
    "import torchtext.vocab\n",
    "from collections import OrderedDict, Counter\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Dict, Callable, Tuple, List\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import vocab as vc\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def collate_func(\n",
    "        samples: List[Tuple[str, str]],\n",
    "        vocab: torchtext.vocab.Vocab, tokenizer: Callable, max_len: int,\n",
    "        class_index_mapping: Dict[str, int]\n",
    ") -> Dict[str, torch.tensor]:\n",
    "    \"\"\"Collate function for training / validation / testing.\n",
    "\n",
    "    :param samples: A list of (text, label).\n",
    "    :param vocab: Vocabulary.\n",
    "    :param tokenizer: Tokenizer.\n",
    "    :param max_len: Max len of the sentences.\n",
    "    :param class_index_mapping: Mapping from label string to label index.\n",
    "    :return: Collated batch.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = list(zip(*samples))\n",
    "\n",
    "    texts = torch.tensor(list(map(\n",
    "        lambda text: pad(\n",
    "            vocab(tokenizer(text)),\n",
    "            max_len=max_len\n",
    "        ),\n",
    "        texts\n",
    "    )))\n",
    "\n",
    "    labels = torch.tensor(list(map(\n",
    "        lambda label: class_index_mapping[label],\n",
    "        labels\n",
    "    )))\n",
    "\n",
    "    return {\n",
    "        \"texts\": texts,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "class AGNewsDataset(Dataset):\n",
    "    \"\"\"Dataset for AG News.\n",
    "\n",
    "    :param fp: File path of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, fp: str):\n",
    "        self.texts, self.labels = self.read(fp)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[str, str]:\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return text, label\n",
    "\n",
    "    @classmethod\n",
    "    def read(cls, fp: str) -> Tuple[List[str], List[str]]:\n",
    "        \"\"\"Obtain texts and labels from the data file.\n",
    "\n",
    "        :param fp: File path of the data.\n",
    "        :return: texts and labels.\n",
    "        \"\"\"\n",
    "\n",
    "        texts = []\n",
    "        labels = []\n",
    "        with open(fp, encoding=\"utf-8\") as f:\n",
    "            csv_reader = csv.reader(f)\n",
    "            for line in csv_reader:\n",
    "                label, title, body = line\n",
    "\n",
    "                # Concatenate the title and the body as text.\n",
    "                texts.append(f\"{title} {body}\")\n",
    "                labels.append(label)\n",
    "\n",
    "                # TODO: Some operations in collate_func can be placed here.\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    import torch\n",
    "\n",
    "\n",
    "class RNNTextClassifier(nn.Module):\n",
    "    \"\"\"A text classifier based on RNN.\n",
    "\n",
    "    :param emb_len: Embedding dimension.\n",
    "    :param hid_dim: Dimension of the RNN hidden layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_len: int, class_num: int, embed_dim: int, hidden_dim: int):\n",
    "        super(RNNTextClassifier, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_len,\n",
    "            embedding_dim=self.embed_dim\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.embed_dim,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=self.hidden_dim,\n",
    "            out_features=class_num\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        embeddings = self.embedding(x)\n",
    "\n",
    "        _, last_hidden = self.rnn(embeddings)\n",
    "        last_hidden = last_hidden.squeeze(dim=0)\n",
    "\n",
    "        y = self.linear(last_hidden)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "def make_class_index_mapping(labels: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"Make a mapping that maps the classes to integral indices.\n",
    "\n",
    "    :param labels: Label strings.\n",
    "    :return: Label indices.\n",
    "    \"\"\"\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    class_index_mapping = {\n",
    "        class_: idx\n",
    "        for idx, class_ in enumerate(classes)\n",
    "    }\n",
    "\n",
    "    return class_index_mapping\n",
    "\n",
    "\n",
    "def make_vocab(\n",
    "        texts: List[str], tokenizer: Callable,\n",
    "        min_freq: int = 1, unk_token: str = \"<unk>\", unk_idx: int = 0,\n",
    ") -> torchtext.vocab.Vocab:\n",
    "    \"\"\"Make a vocabulary from the specified texts.\n",
    "\n",
    "    :param texts: Texts for making vocab.\n",
    "    :param tokenizer: Tokenizer for tokenizing texts.\n",
    "    :param min_freq: Min frequency of the words to be added to the vocab.\n",
    "    :param unk_token: Unknown token.\n",
    "    :param unk_idx: Unknown token index.\n",
    "    :return: Constructed vocab.\n",
    "    \"\"\"\n",
    "\n",
    "    def make_token_count_mapping() -> Dict[str, int]:\n",
    "        \"\"\"Make a mapping {token: count}\"\"\"\n",
    "\n",
    "        all_tokens = [\n",
    "            token\n",
    "            for text in texts\n",
    "            for token in tokenizer(text)\n",
    "        ]\n",
    "        counter = Counter(all_tokens)\n",
    "        counter = sorted(\n",
    "            counter.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True\n",
    "        )\n",
    "        counter = OrderedDict(counter)  # Not necessary for python >= 3.6\n",
    "\n",
    "        return counter\n",
    "\n",
    "    vocab = vc(\n",
    "        ordered_dict=make_token_count_mapping(),\n",
    "        min_freq=min_freq,\n",
    "    )\n",
    "    vocab.insert_token(\n",
    "        token=unk_token,\n",
    "        index=unk_idx\n",
    "    )\n",
    "    vocab.set_default_index(index=unk_idx)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def pad(token_indices: List[int], max_len: int, default_pad_val: int = 0) -> List[int]:\n",
    "    \"\"\"Pad the given token indices.\n",
    "\n",
    "    :param token_indices: Indices of the tokens.\n",
    "    :param max_len: Max sentence length.\n",
    "    :param default_pad_val: Default padding value.\n",
    "    :return: Padded indices.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(token_indices) < max_len:\n",
    "        return token_indices + ([default_pad_val] * (max_len - len(token_indices)))\n",
    "    else:\n",
    "        return token_indices[:max_len]\n",
    "\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        texts = batch[\"texts\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        predictions = model(texts)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = torch.tensor(losses).mean()\n",
    "    print(f\"Train Loss : {train_loss:.3f}\")\n",
    "\n",
    "\n",
    "def valid(model, criterion, dev_loader):\n",
    "    \"\"\"Validate the model performance.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []  # Store all labels in the dev data.\n",
    "    all_predictions = []  # Store all predictions.\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            texts = batch[\"texts\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)  # labels here: labels in a batch.\n",
    "\n",
    "            predictions = model(texts)  # predictions here: predictions in a batch.\n",
    "\n",
    "            loss = criterion(predictions, labels)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predictions.argmax(dim=-1))\n",
    "\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    valid_loss = torch.tensor(losses).mean()\n",
    "    valid_acc = accuracy_score(\n",
    "        y_true=all_labels.detach().cpu().numpy(),\n",
    "        y_pred=all_predictions.detach().cpu().numpy()\n",
    "    )\n",
    "    print(f\"Valid Loss : {valid_loss:.3f}\")\n",
    "    print(f\"Valid Acc  : {valid_acc:.3f}\")\n",
    "\n",
    "    return valid_loss.item()\n",
    "\n",
    "\n",
    "def test(model, test_loader, class_index_mapping):\n",
    "    \"\"\"Test the model\"\"\"\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            texts = batch[\"texts\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            predictions = model(texts)\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predictions)\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "    all_labels = all_labels.detach().cpu().numpy()\n",
    "    all_predictions = F.softmax(all_predictions, dim=-1).argmax(dim=-1).detach().cpu().numpy()\n",
    "\n",
    "    test_acc = accuracy_score(\n",
    "        y_true=all_labels,\n",
    "        y_pred=all_predictions\n",
    "    )\n",
    "    print(f\"Test Acc   : {test_acc:.3f}\")\n",
    "\n",
    "    print(\"\\nClassification Report : \")\n",
    "    print(classification_report(all_labels, all_predictions, target_names=class_index_mapping.keys()))\n",
    "\n",
    "    print(\"\\nConfusion Matrix : \")\n",
    "    print(confusion_matrix(all_labels, all_predictions))\n",
    "\n",
    "\n",
    "def predict(model, text, tokenizer, vocab, class_index_mapping):\n",
    "    \"\"\"Predict the label of the given text.\"\"\"\n",
    "\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = vocab(tokens)\n",
    "    texts = torch.tensor([token_ids])\n",
    "    with torch.no_grad():\n",
    "        predictions = model(texts)\n",
    "\n",
    "    prediction_index = predictions[0].argmax(dim=0).item()\n",
    "    prediction_class = {\n",
    "        index: class_\n",
    "        for class_, index in class_index_mapping.items()\n",
    "    }[prediction_index]\n",
    "\n",
    "    print(f\"\\ntext: {text}\")\n",
    "    print(f\"prediction: {prediction_class}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_dataset = AGNewsDataset(fp=\"../train.csv\")\n",
    "    dev_dataset = AGNewsDataset(fp=\"../dev.csv\")\n",
    "    test_dataset = AGNewsDataset(fp=\"../test.csv\")\n",
    "\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    # tokenizer = get_tokenizer(word_tokenize)\n",
    "    vocab = make_vocab(\n",
    "        texts=train_dataset.texts,\n",
    "        tokenizer=tokenizer,\n",
    "        min_freq=min_freq,\n",
    "    )\n",
    "    class_index_mapping = make_class_index_mapping(labels=train_dataset.labels)\n",
    "\n",
    "    model = RNNTextClassifier(\n",
    "        vocab_len=len(vocab),\n",
    "        class_num=len(class_index_mapping),\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "    )\n",
    "    model.to(device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(\n",
    "        params=model.parameters(),\n",
    "        lr=lr,\n",
    "    )\n",
    "\n",
    "    collate_fn = lambda samples: collate_func(\n",
    "        samples=samples,\n",
    "        tokenizer=tokenizer,\n",
    "        vocab=vocab,\n",
    "        max_len=max_len,\n",
    "        class_index_mapping=class_index_mapping,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_bz,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dev_loader = DataLoader(\n",
    "        dev_dataset,\n",
    "        batch_size=eval_bz,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=eval_bz,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    best_model = None\n",
    "    for i in range(num_epochs):\n",
    "        print(f\"Epoch {i + 1}\")\n",
    "\n",
    "        train(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            train_loader=train_loader\n",
    "        )\n",
    "\n",
    "        valid_loss = valid(\n",
    "            model=model,\n",
    "            criterion=criterion,\n",
    "            dev_loader=dev_loader\n",
    "        )\n",
    "        # Update the best model.\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    test(\n",
    "        model=best_model,\n",
    "        test_loader=test_loader,\n",
    "        class_index_mapping=class_index_mapping\n",
    "    )\n",
    "\n",
    "    predict(\n",
    "        model=best_model,\n",
    "        text=\"Apple Tops in Customer Satisfaction \\\"Dell comes in a close second, while Gateway shows improvement, study says.\\\"\",\n",
    "        tokenizer=tokenizer,\n",
    "        vocab=vocab,\n",
    "        class_index_mapping=class_index_mapping,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Data args.\n",
    "    min_freq = 10  # Min frequency of the word added to the vocab.\n",
    "    max_len = 25  # Max sentence len.\n",
    "    # Model args.\n",
    "    embed_dim = 50  # Embedding dimension.\n",
    "    hidden_dim = 50  # Hidden dimension of the RNN.\n",
    "    # Training args.\n",
    "    num_epochs = 10  # Number of training epochs.\n",
    "    lr = 1e-3  # Learning rate.\n",
    "    train_bz = 64  # Training batch size.\n",
    "    eval_bz = 64  # Validation/testing batch size.\n",
    "    \n",
    "    device = torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
